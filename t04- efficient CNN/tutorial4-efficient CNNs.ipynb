{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "\n",
    "\n",
    "# CS236781: Deep Learning\n",
    "# Tutorial 4: Efficient and special CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "- Recup over resnets\n",
    "- Batch Normalization\n",
    "- SqueezeNet\n",
    "- Depthwise Separable Convolutions\n",
    "- MobileNet\n",
    "- MobileNet v2\n",
    "- MobileNet v3\n",
    "- ShuffleNet \n",
    "- EfficientNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-24T07:23:40.304945Z",
     "iopub.status.busy": "2022-03-24T07:23:40.300065Z",
     "iopub.status.idle": "2022-03-24T07:23:41.606026Z",
     "shell.execute_reply": "2022-03-24T07:23:41.605724Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-24T07:23:41.608025Z",
     "iopub.status.busy": "2022-03-24T07:23:41.607915Z",
     "iopub.status.idle": "2022-03-24T07:23:41.621298Z",
     "shell.execute_reply": "2022-03-24T07:23:41.621040Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theory Reminders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Convolution neural networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/arch.png\" width=\"500\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Resnet\n",
    "\n",
    "<center><img src=\"img/resnet_block2.png\" width=\"900\"/></center>\n",
    "\n",
    "(Left: basic block; right: bottleneck block).\n",
    "\n",
    "Here the weight layers are `3x3` or `1x1` convolutions followed by batch-normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/rn.webp\" width=\"900\"/></center>\n",
    "\n",
    "\n",
    "<center><img src=\"img/resnet_arch_table.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Batch normalization is a technique for improving the speed, performance, and stability of deep neural networks.\n",
    "\n",
    "It is used to normalize the input layer by adjusting and scaling the activations.\n",
    "BN works for all pixels of the feature - channel-wize ( $\\in \\mathcal{R}^c$).\n",
    "\n",
    "The original goal was to accelerating training by reducing **Internal Covariate Shift**\n",
    "\n",
    "**Covariate Shift** is when the distribution of input data shifts between the training environment and live environment.\n",
    "\n",
    "reducing the shift- i.e, make not only the input be normal distributed but also the intermidiate features, shuld accelerate the learning process.\n",
    "\n",
    "**Note**: it came in 2015 and not only accelerated models, but converge to a better minima of the loss function.\n",
    "A lot of experiments have been made since and hypothesis of why is it work have been made.\n",
    "\n",
    "We do not have a clear explanation untill today, but studies shown that it improve the Lipschitzness of both the loss and the gradients.\n",
    "\n",
    "In other words, it creates a smoother loss manifold that is easier to optimize the hypotesis over.\n",
    "<img src=\"img/bn_5.png\" width=\"400\" alt=\"scale\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Batch Norm** has 4 groups of parameters, two are learnable and the other are statistical from the data\n",
    "<center><img src=\"img/bn_1.png\" width=\"700\" alt=\"scale\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### During trainig\n",
    "<center><img src=\"img/bn_2.png\" width=\"700\" alt=\"scale\"></center>\n",
    "\n",
    "the momentum $\\alpha = 0.9$ and epsilon is 1e-5 to avoid devision by 0\n",
    "\n",
    "But at test time, we cannot use the statistics of a batch to classify (why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### During test\n",
    "<center><img src=\"img/bn_4.png\" width=\"700\" alt=\"scale\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# A function to count the number of parameters in an nn.Module.\n",
    "def num_params(layer):\n",
    "    return sum([p.numel() for p in layer.parameters()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's use torch BN2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1: 20 parameters\n"
     ]
    }
   ],
   "source": [
    "# First conv layer: works on input image volume\n",
    "bn1 = nn.BatchNorm2d(10)\n",
    "print(f'bn1: {num_params(bn1)} parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What would be the output shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 32, 32])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fm = torch.randn(1,10,32,32)\n",
    "output_fm = bn1(input_fm)\n",
    "output_fm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Efficiency in nueral networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-24T07:23:42.496857Z",
     "iopub.status.busy": "2022-03-24T07:23:42.496774Z",
     "iopub.status.idle": "2022-03-24T07:23:42.511150Z",
     "shell.execute_reply": "2022-03-24T07:23:42.510875Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Besides of creating better models, the deep learning revolution came to edge devices, like phones, drones, cars and more.\n",
    "\n",
    "The race to create compact yet sufficient models has began.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Separable Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We already know that convolution operation is a sparse matrix multiplication.\n",
    "\n",
    "What if we could decompose this matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spatially Separable Convolutions\n",
    "\n",
    "The spatially separable convolution operates on the 2D spatial dimensions of images, i.e. height and width. Conceptually, spatially separable convolution decomposes a convolution into two separate operations. For an example shown below, a Sobel kernel, which is a 3x3 kernel, is divided into a 3x1 and 1x3 kernel.\n",
    "\n",
    "<center><img src=\"img/sobel.png\" width=\"700\" alt=\"scale\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Naturally, it would look like that:\n",
    "\n",
    "<center><img src=\"img/SSC.png\" width=\"700\" alt=\"scale\"></center>\n",
    "\n",
    "And while 1xHxW input image with fXf convolution would do (no padding) $(H-2)X(W-2)XfXf$ multiplications.\n",
    "\n",
    "Spatially Separable Convolutions would do $(H-2) X W X f  +  (H-2)X(W-2)Xf $ multiplications.\n",
    "\n",
    "The payment is the types of convolution we can represent and can be decomposed like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Depthwise Separable Convolutions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Another way to reduce computation complexity is to devide the input channels per convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Depthwise Convolution\n",
    "\n",
    "Let's look at 128 convolution kernels of 3x3 with input of size 3x7x7:\n",
    "\n",
    "<center><img src=\"img/conv1.png\" width=\"700\" alt=\"scale\"></center>\n",
    "\n",
    "we see that each channel of the output contain information from all input channels.\n",
    "\n",
    "However, we can devide each input channel seperatly and convolve kernels with a dept of 1:\n",
    "\n",
    "\n",
    "<center><img src=\"img/conv_dept.png\" width=\"700\" alt=\"scale\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What seems to be the problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to aggregate the information and create a rich feature map, we add anothe step of 1x1 conv that expand the feature map:\n",
    "\n",
    "<center><img src=\"img/conv_dept2.png\" width=\"700\" alt=\"scale\"></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class depthwise_separable_conv(nn.Module):\n",
    "    def __init__(self, nin, kernels_per_layer, nout): \n",
    "        super(depthwise_separable_conv, self).__init__() \n",
    "        self.depthwise = nn.Conv2d(nin, nin * kernels_per_layer, kernel_size=3, padding=1, groups=nin) \n",
    "        self.pointwise = nn.Conv2d(nin * kernels_per_layer, nout, kernel_size=1) \n",
    "    def forward(self, x): \n",
    "        out = self.depthwise(x) \n",
    "        out = self.pointwise(out) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 10, 32, 32])\n",
      "output shape: torch.Size([1, 128, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "conv = nn.Conv2d(10,3,128)\n",
    "dsc = depthwise_separable_conv(input_fm.shape[1],3,128)\n",
    "print(f'input shape: {input_fm.shape}')\n",
    "print(f'output shape: {dsc(input_fm).shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple convolution has: 491523 parameters\n",
      "depthwise separable conv has: 4268 parameters\n"
     ]
    }
   ],
   "source": [
    "print(f'simple convolution has: {num_params(conv)} parameters')\n",
    "print(f'depthwise separable conv has: {num_params(dsc)} parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SqueezeNet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-24T07:23:42.512891Z",
     "iopub.status.busy": "2022-03-24T07:23:42.512788Z",
     "iopub.status.idle": "2022-03-24T07:23:42.532553Z",
     "shell.execute_reply": "2022-03-24T07:23:42.532226Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First attempt to make a mobile device friently model\n",
    "\n",
    "The main building block in SqueezeNet is the **Fire module**\n",
    "\n",
    "\n",
    "<center><img src=\"img/SqueezeNet.png\" width=\"500\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It first has a **squeeze layer**. This is a 1×1 convolution that reduces the number of channels, for example from 64 to 16 in the above picture. The purpose of the squeeze layer is to compress the data, so that the 3×3 convolution doesn’t need to learn so many parameters.\n",
    "\n",
    "This is followed by an **expand block** that has two parallel convolution layers: one with a 1×1 kernel, the other with a 3×3 kernel. These conv layers also increase the number of channels again, from 16 back to 64. Their outputs are concatenated, so the output of this fire module has 128 channels in total.\n",
    "\n",
    "The idea exist in Resnet bottleneck blocks as well, yet the model has about 10% from the Resnet18 parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Preformence (ImageNet 1K):\n",
    "\n",
    "* Accuracy: 57.5%\n",
    "\n",
    "* Parameters 1.25M \n",
    "\n",
    "to compare to other networks: (Resnet18 - 11.75M, Resnet50 - 23.5M, VGG16 - 134.7M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MobileNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The main idea, was to use **Depthwise Separable Convolutions** instead of the expensive regular ones..\n",
    "\n",
    "MobileNet v1 consists of 13 convolution blocks in a row. It does not use max pooling to reduce the spatial dimensions, but some of the depthwise layers have stride 2.\n",
    "\n",
    "At the end is a global average pooling layer followed by a fully-connected layer.\n",
    "\n",
    "Often ReLU6 is used as the activation function instead of plain old ReLU.\n",
    "\n",
    "What is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0, x)\n",
    "def relu6(x):\n",
    "    return min(max(0, x), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Preformence (ImageNet 1K):\n",
    "\n",
    "* Accuracy: 70.9%\n",
    "\n",
    "* Parameters 4.2M \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mobilenet V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall that in Resnet, we had bottleneck blocks.\n",
    "\n",
    "Those goes as wide->narrow->wide in order to reduce the computational complexity.\n",
    "\n",
    "In Mobilenet, we saq that DSC breaks down the complexity, and basically do not benefit that greatly from this setup.\n",
    "\n",
    "Mobilenet V2 introduced **Inverted Residuals**, as the blocks are narrow->wide->narrow:\n",
    "\n",
    "\n",
    "<center><img src=\"img/IRB.png\" width=\"800\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The authors describe this idea as an inverted residual block because skip connections exist between narrow parts of the network which is opposite of how an original residual connection works.\n",
    "\n",
    "#### Linear bottlenecks:\n",
    "\n",
    "The reason we use non-linear activation functions in neural networks is that multiple matrix multiplications cannot be reduced to a single numerical operation. It allows us to build neural networks that have multiple layers. At the same time the activation function ReLU, which is commonly used in neural networks, discards values that are smaller than 0. This loss of information can be tackled by increasing the number of channels in order to increase the capacity of the network.\n",
    "\n",
    "With inverted residual blocks we do the opposite and squeeze the layers where the skip connections are linked. This hurts the performance of the network. The authors introduced the idea of a linear bottleneck where the last convolution of a residual block has a linear output before it’s added to the initial activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Preformence (ImageNet 1K):\n",
    "\n",
    "* Accuracy: 71.8%\n",
    "\n",
    "* Parameters 3.47M "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MobileNet V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Improvement over V2 with some tricks:\n",
    "\n",
    "* uses also [Squeeze-and-Excitation Networks](https://arxiv.org/pdf/1709.01507.pdf)\n",
    "* Neural Architecture Search for Block-Wise Search\n",
    "* [NetAdapt](https://arxiv.org/pdf/1804.03230.pdf) for Layer wise search\n",
    "* Network Improvements — Layer removal and H-swish\n",
    "\n",
    "$Hswish(x) = x \\frac{ReLU6(x+3)}{6}$\n",
    "\n",
    "<center><img src=\"img/swish.png\" width=\"600\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Preformence (ImageNet 1K):\n",
    "\n",
    "#### small:\n",
    "* Accuracy: 67.5%\n",
    "* Parameters 2.9M\n",
    "\n",
    "#### large:\n",
    "* Accuracy: 75.2%\n",
    "* Parameters 5.4M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ShuffleNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Many of the modern architectures use lots of dense 1×1 convolutions, also known as pointwise convolutions, but they can be relatively expensive. To bring down this cost, we can use group convolutions on those layers. But those have side effects, with can be mitigated using a channel shuffle operation.\n",
    "\n",
    "A group-wise convolution divides the input feature maps into two or more groups in the channel dimension, and performs convolution separately on each group. It is the same as slicing the input into several feature maps of smaller depth, and then running a different convolution on each.\n",
    "\n",
    "<center><img src=\"img/ChannelShuffle.png\" width=\"600\" /></center>\n",
    "\n",
    "\n",
    "you can read more in [here](https://arxiv.org/pdf/1707.01083.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Preformence (ImageNet 1K):\n",
    "\n",
    "#### V1:\n",
    "* Accuracy: 69.4%\n",
    "* Parameters 2.3M\n",
    "\n",
    "#### V2 large:\n",
    "* Accuracy: 77.1%\n",
    "* Parameters 6.7M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## EfficientNet: The King of fixed solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Build by **AutoML NAS** framework\n",
    "\n",
    "The network is fine-tuned for obtaining maximum accuracy but is also penalized if the network is very computationally heavy.\n",
    "\n",
    "It is also penalized for slow inference time when the network takes a lot of time to make predictions. The architecture uses a mobile inverted bottleneck convolution similar to MobileNet V2 but is much larger due to the increase in FLOPS. This baseline model is scaled up to obtain the family of EfficientNets.\n",
    "\n",
    "  <center><img src=\"img/eff.png\" width=\"600\" /></center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And the second version:\n",
    "\n",
    "  <center><img src=\"img/effv2.png\" width=\"1100\" /></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model was build using more advanced NAS methods.\n",
    "\n",
    "Original code for [V1](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet) and [V2](https://github.com/google/automl/tree/master/efficientnetv2)\n",
    "\n",
    "recommanded implementations in pytorch (since google don't like us...) can be found in\n",
    "[Timm](https://github.com/rwightman/pytorch-image-models/blob/main/timm/models/efficientnet.py) or [Luke Melas-Kyriazi](https://github.com/lukemelas/EfficientNet-PyTorch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### custom solutions\n",
    "\n",
    "Today, we have a good understanding that edge devices have diffrent HW architecture and we might want to build a custom solutions.\n",
    "\n",
    "Many works uses NAS with specific hardware constraint in order to find a fast and good solution.\n",
    "\n",
    "Other methods like compration, quantization and pruning exist in order to accelerate the inference or trainig time.\n",
    "\n",
    "If you're intrested in efficient , contact me after class :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Thanks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Credits**\n",
    "\n",
    "This tutorial was written by [Moshe Kimhi](https://www.linkedin.com/in/moshekimhi/).<br>\n",
    "To re-use, please provide attribution and link to the original.\n",
    "\n",
    "Some images in this tutorial were taken and/or adapted from the following sources:\n",
    "\n",
    "- Sebastian Raschka, https://sebastianraschka.com/\n",
    "- Deep Learning, Goodfellow, Bengio and Courville, MIT Press, 2016\n",
    "- Fundamentals of Deep Learning, Nikhil Buduma, Oreilly 2017\n",
    "- Deep Learning with Python, Francios Chollet, Manning 2018\n",
    "- Stanford cs231n course notes by Andrej Karpathy\n",
    "- Ketan Doshi on medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
