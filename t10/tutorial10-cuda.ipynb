{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\Tr}[0]{^\\top}\n",
    "\\newcommand{\\softmax}[1]{\\mathrm{softmax}\\left({#1}\\right)}\n",
    "$$\n",
    "\n",
    "# CS236781: Deep Learning\n",
    "# Tutorial 10: CUDA Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "- The CUDA programming model\n",
    "- Accelerating numerical python code with `numba`\n",
    "- Implementing CUDA kernels in python\n",
    "- Thread synchronization\n",
    "- Shared memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CUDA programming model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA is a parallel programming model and software environment that leverages the computational resources of NVIDIA GPU's for general-purpose numeric computation.\n",
    "\n",
    "It provides compilers, programming-language extensions, optimized software libraries and developer tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CUDA defines a programming model and a memory model\n",
    "- CUDA programs run 1000's of threds on on 100's of physical cores\n",
    "- Defines extensions to C language to write GPU code (But here we'll use Python :)\n",
    "- Allows heterogeneous computation:\n",
    "    - CPU runs sequential operations and invokes GPU\n",
    "    - GPU runs massively-parallel work\n",
    "    - Both can run concurrently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Device**: The GPU  \n",
    "**Host**: The machine controlling the GPU\n",
    "\n",
    "| Heterogeneous computing | Host-device communication |\n",
    "| --| --|\n",
    "| <center><img src=\"img/hetero.png\" width=\"300\" /></center> | <center><img src=\"img/host_device.png\" width=\"700\" /></center>|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A **Kernel** is a function that is *called from host* and *executes on device*\n",
    "- Generally, one kernel executes at a time on the entire device\n",
    "    - Actually, kernels can be queued into \"streams\"\n",
    "    - Kernels from different streams can overlap\n",
    "- A Kernel runs with using many concurrent threads\n",
    "- Each thread executes the *same code*\n",
    "\n",
    "<center><img src=\"img/kernel.png\" width=\"300\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel \"Geometry\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kernel launches as a 1d or 2d-**grid** of **thread blocks**\n",
    "- Each **block** contains multiple threads arranged in a 1d, 2d or 3d configuration\n",
    "- Threads within a block can synchronize (barrier) and share memory\n",
    "- Each thread has a **unique id** that is mostly used for\n",
    "    - Selecting in/out data (computing memory access locations)\n",
    "    - Control-flow decisions\n",
    "    \n",
    "<center><img src=\"img/kernel_geom.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that multi-dimensional grids and blocks are just for the convenience of the programmer.\n",
    "- Helps implement algorithms for 2d and 3d data\n",
    "- Nothing actually changes in the hardware execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is a kernel implemented and launched?\n",
    "\n",
    "- The CUDA C-extensions allow the programmer to define which code is compiled for CPU or GPU.\n",
    "- A special syntax (`<<< >>>`) allows the definition of kernel geometry when launching it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "__global__ void MyKernel() {}      // call from host, execute on GPU\n",
    "__device__ float MyDeviceFunc() {} // call from GPU, execute on GPU\n",
    "__host__ int HostFunc() {}         // call from host, execute on host\n",
    "\n",
    "dim3 dimGrid(100, 50);  // 5000 thread blocks in the grid, in a 2D layout\n",
    "dim3 dimBlock(4, 8, 8); // 256 threads per block, in a 3D layout\n",
    "MyKernel <<< dimGrid, dimBlock >>> (...); // Launch kernel\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Practically zero creation and switching overhead\n",
    "- Can launch kernels with thousands of threads, many more than physical cores (**\"oversubscribed\"**)\n",
    "    - When a thread is blocked due to memory latency, it's instantly swapped out with another waiting thread\n",
    "    - Instant thread switching hides memory latency\n",
    "- Even very simple kernels can generate performance benefit with massive parallelization\n",
    "- Scheduled together in \"warps\": groups of (usually 32) threads performing the same instruction (SIMT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Thread IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CUDA runtime provides **special variables** for determining the geometry of the currently executing kernel:\n",
    "- `gridDim`: Dimensions of the grid, in blocks. Can be 1d or 2d.\n",
    "- `blockDim`: Dimension of the block, in threads. Can be 1d, 2d, or 3d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CUDA runtime provides **special variables** for calculating the unique thread id:\n",
    "- `blockIdx`: Index of current block, within the grid. Can be 1d or 2d.\n",
    "- `threadIdx`: Index of current thread, within the block. Can be 1d, 2d, or 3d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: How can we use the above variables to obtain the unique thread id?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A unique thread id for a 1d kernel geometry can be obtained with `blockIdx.x * blockDim.x + threadIdx.x`.\n",
    "\n",
    "<center><img src=\"img/thread_id_1d.png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key idea of CUDA\n",
    "\n",
    "- Write a single-threaded program with the **thread id** as a parameter.\n",
    "- Use thread id to select a subset of data to process.\n",
    "- Launch many threads, so that together they cover the entire dataset.\n",
    "- Code automatically to all available physical processors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key feature of CUDA is that a Kernel transparently scales to device with a different number of physical processors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A thread is executed by a single CUDA core.\n",
    "- A thread block is executed within a one \"streaming multiprocessor\" (SM), containing multiple CUDA cores.\n",
    "- The entire grid is executed on a device, which contains many SMs.\n",
    "\n",
    "<center><img src=\"img/execution_model.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hardware automatically schedules thread blocks on any available multiprocessor.\n",
    "\n",
    "Source code defining kernel \"geometry\" stays the same regardless of hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the same Kernel configuration can be launched on devices with a different number of multiprocessors:\n",
    "<center><img src=\"img/block_scheduling.png\" width=\"900\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different types of memory are available to device threads.\n",
    "\n",
    "The most important ones are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Registers**\n",
    "- Per-thread access\n",
    "- On chip $\\rightarrow$ extremely fast\n",
    "- Persisted until thread terminates\n",
    "\n",
    "**Thread-local memory**\n",
    "- Stores per-thread local variables that cannot fit in the register memory\n",
    "- Located in DRAM $\\rightarrow$ slow\n",
    "- Persisted until thread terminates\n",
    "    \n",
    "<center><img src=\"img/mem_local.png\" width=\"330\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shared memory**\n",
    "- Shared between threads in the same thread block\n",
    "- Used for collaboration between threads in the same block\n",
    "- On chip $\\rightarrow$ very fast\n",
    "- Persisted until end of block\n",
    "    \n",
    "<center><img src=\"img/mem_shared.png\" width=\"273\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global memory**\n",
    "- Can be access by any thread in any thread block\n",
    "- Used to copy to/from host\n",
    "- Located in DRAM $\\rightarrow$ slow\n",
    "- Persisted for the life of the application\n",
    "\n",
    "<center><img src=\"img/mem_global.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristics for Kernel sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many blocks?\n",
    "- Should occupy every SM $\\rightarrow$ At least one block per SM\n",
    "- Should have something to run on SM if current block is waiting (e.g. sync) $\\rightarrow$ At least two blocks per SM\n",
    "- Should scale with same code if we upgrade hardware $\\rightarrow$ Many blocks per SM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many threads?\n",
    "- Many threads $\\rightarrow$ hides global memory latency\n",
    "- Too many threads $\\rightarrow$ exhaust registers and shared memory\n",
    "- Should be a multiple of warp size\n",
    "- Typical selection: 64 to 512 per block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing CUDA Kernels with `numba`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is `numba`?\n",
    "\n",
    "Numba is a **just-in-time** (JIT) **function compiler**, focused on **numerical python**.\n",
    "It can be used to accelerate python code by generating efficient, **type-specialized** machine code.\n",
    "\n",
    "Numba supports all major OSes and a wide range of hardware (Intel x86/64, NVIDIA CUDA, ARM).\n",
    "It's developed and actively maintained by Anaconda Inc., and considered production ready.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explain the terms we used above:\n",
    "\n",
    "**Just-in-time**: Functions are compiled the first time they're called.  The compiler therefore knows the argument types.\n",
    "\n",
    "Bonus: This also allows Numba to be used interactively in a Jupyter notebook :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function compiler**:  Numba compiles Python functions, not entire applications.\n",
    "\n",
    "Numba does not replace the Python interpreter, it effectively transforms a function into a usually faster function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical python**: Numba supports only a subset of the python language. It works well with numerical types such as `int`, `float`, and `complex`, functions from the `math` and `cmath` modules and with `numpy` arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Type-specialized**: Numba speeds up your function by generating a specialized implementation for the specific data types you are using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/numba_flowchart.png\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First steps with `numba` on the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a \"Hello World\" style example: A trivial function that increments an array by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def inc_cpu(a):\n",
    "    for i in range(len(a)):\n",
    "        a[i] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `numba.jit` decorator to wrap our code in a `numba` object that will JIT-compile and cache it when called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CPUDispatcher(<function inc_cpu at 0x7ff9c1e97c20>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The inc_cpu variable no longer points to a regular python function, but a callable wrapper.\n",
    "inc_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the `nopython` option?\n",
    "- If `nopython=True`, `numba` will try to compile the entire function so that it can be run completely without the Python interpreter. This is usually what you want.\n",
    "- Otherwise, `numba` will try to compile the entire function, but if there are unsupported operations or types it will try to only extract loops and compile them as separate functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a million-element array and see how fast the python interpreter is using `%timeit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((10**6,), dtype=np.float32)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.92 s ± 22 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([8., 8., 8., ..., 8., 8., 8.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run as regular python code (interpreted)\n",
    "%timeit inc_cpu.py_func(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had to call the function with `.py_func` to get the original function (before wrapping with the jitter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call it though the wrapper to time the compiled version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268 µs ± 4.02 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([16., 16., 16., ..., 16., 16., 16.], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run as jit-compiled machine code\n",
    "%timeit inc_cpu(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's about 5 orders of magnitude faster! Not bad for just adding a decorator function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242 µs ± 4.43 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Run using numpy add(), this is like a + 1 but without allocating output array\n",
    "%timeit np.add(a, 1, out=a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we get results similar to `numpy`'s optimized C code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important note about benchmarking:\n",
    "\n",
    "The first time we called `inc_cpu` we paid a overhead price for the compilation.\n",
    "However, the `%timeit` magic returns the best result from multiple runs, so our results do not show this overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First steps with `numba` on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA devices\n",
      "id 0    b'GeForce RTX 2080 Ti'                              [SUPPORTED]\n",
      "                      compute capability: 7.5\n",
      "                           pci device id: 0\n",
      "                              pci bus id: 15\n",
      "Summary:\n",
      "\t1/1 devices are supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numba import cuda\n",
    "\n",
    "# Show GPUs on the machine\n",
    "cuda.detect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rewrite our \"Hello World\" example as a CUDA kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def inc_gpu(a):\n",
    "    # Compute unique thread id\n",
    "    idx = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    \n",
    "    # Notice:\n",
    "    # 1. No loop: every thread will operate on a single element\n",
    "    # 2. We assume more threads than array elements\n",
    "    if idx < a.shape[0]:\n",
    "        a[idx] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets invoke this kernel with a specific geometry containing more threads than array elements (over 1M threads!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#blocks=3907, #threads per block=256\n"
     ]
    }
   ],
   "source": [
    "blocksize = 256\n",
    "gridsize = math.ceil(a.shape[0] / blocksize)\n",
    "print(f'#blocks={gridsize}, #threads per block={blocksize}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185 µs ± 335 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([89238., 89238., 89238., ..., 89238., 89238., 89238.], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy data to GPU memory\n",
    "d_a = cuda.to_device(a)\n",
    "\n",
    "# Run as a kernel on GPU\n",
    "# Note that we must synchronize to benchmark properly\n",
    "%timeit inc_gpu[gridsize, blocksize](d_a); cuda.synchronize()\n",
    "\n",
    "# Copying data back from device will also synchronize, i.e. wait for kernel to complete\n",
    "a = d_a.copy_to_host()\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the `cuda.synchronize()` about? Why do we need it for the benchmark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launching a kernel is non-blocking: A CUDA kernel executes concurrently with the host code.\n",
    "\n",
    "The host can execute multiple kernels which will be serialized (as a \"stream\") and call `synchronize()` to block until their completion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, note that copying memory to/from the host is a blocking operation. \n",
    "If we do not manually copy, `numba` will do this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.29 ms ± 6.85 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Invoke the kernel on data in HOST memory\n",
    "# Numba with automatically copy, synch and copy back\n",
    "%timeit inc_gpu[gridsize, blocksize](a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why so slow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now included two memory copies in each benchmark iteration!\n",
    "So this is not a correct comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input size independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we got great performance with our simple CUDA kernel, but there's a major **limitation** with the above kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our grid size depends on the input size - We assumed the grid contains a thread for every single input element.\n",
    "\n",
    "This is inconvenient: we'll need to launch the kernel differently for every input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((10**6,), dtype=np.float32)\n",
    "\n",
    "# Not enough threads for all elements\n",
    "inc_gpu[100, 64](a)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How can we fix this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common pattern**: Every thread processes multiple elements, spaced apart by a \"stride\" which jumps over all the threads in the grid.\n",
    "\n",
    "Let's demonstrate the pattern with a kernel that multiplies two arrays elementwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def mult_kernel(a, b, out):\n",
    "    threads_per_block = cuda.blockDim.x\n",
    "    num_blocks = cuda.gridDim.x\n",
    "    \n",
    "    thread_idx_in_block = cuda.threadIdx.x\n",
    "    block_idx = cuda.blockIdx.x\n",
    "   \n",
    "    # Get thread id in the 1d grid, as usual\n",
    "    thread_idx_unique = thread_idx_in_block + block_idx * threads_per_block\n",
    "    \n",
    "    # Calculate range of elements this thread will process\n",
    "    start = thread_idx_unique \n",
    "    end = len(a)\n",
    "    stride = threads_per_block * num_blocks # jump over all threads, in case we have more data than threads\n",
    "    \n",
    "    for i in range(start, end, stride):\n",
    "        out[i] = a[i] * b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_a = cuda.to_device(np.ones((10**6,), dtype=np.float32) * 2)\n",
    "d_b = cuda.to_device(np.ones((10**6,), dtype=np.float32) * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284 µs ± 897 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6., 6., 6., ..., 6., 6., 6.], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_out = cuda.to_device(np.zeros_like(a))\n",
    "\n",
    "# Kernel with a thread for each element\n",
    "%timeit mult_kernel[1024, 1024](d_a, d_b, d_out); cuda.synchronize()\n",
    "\n",
    "d_out.copy_to_host()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317 µs ± 961 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6., 6., 6., ..., 6., 6., 6.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_out = cuda.to_device(np.zeros_like(a))\n",
    "\n",
    "# Less threads than elements\n",
    "%timeit mult_kernel[32, 256](d_a, d_b, d_out); cuda.synchronize()\n",
    "\n",
    "d_out.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Race Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in any multithreaded environment, a race condition occurs when a memory location might be modified by multiple independent threads. For example,\n",
    "\n",
    "- Read-After-Write (RAW): One thread is reading a memory location at the same time another thread might be writing to it.\n",
    "- Write-After-Write (WAW): Two threads are writing to the same memory location, and only one write will be visible when the kernel is complete.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, in our examples, each thread had exclusive responsibility for a unique subsets of output array elements.\n",
    "\n",
    "But what if different threads need to combine results?\n",
    "\n",
    "Let's consider an example, where multiple threads need to increment a global counter:\n",
    "1. Read the current value of a counter in global memory.\n",
    "2. Compute `counter + 1`.\n",
    "3. Write that value back to the counter.\n",
    "\n",
    "CUDA provides \"atomic operations\" which will read, modify and update a memory location in one \"atomic\" operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are simple kernels incrementing a global counter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def counter_kernel(global_counter):\n",
    "    # Race condition\n",
    "    global_counter[0] += 1\n",
    "    \n",
    "@cuda.jit\n",
    "def atomic_counter_kernel(global_counter):\n",
    "    # Add 1 to offset 0 in global_counter array, as an atomic operation\n",
    "    cuda.atomic.add(global_counter, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens with multiple threads running these kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_threads = 1024\n",
    "blocksize = n_threads // 32\n",
    "gridsize = n_threads // blocksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter=1, expected=1024\n"
     ]
    }
   ],
   "source": [
    "counter = cuda.to_device(np.array([0], dtype=np.int32))\n",
    "\n",
    "# Race condition\n",
    "counter_kernel[gridsize, blocksize](counter)\n",
    "\n",
    "print(f'counter={counter.copy_to_host()[0]}, expected={n_threads}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter=1024, expected=1024\n"
     ]
    }
   ],
   "source": [
    "counter = cuda.to_device(np.array([0], dtype=np.int32))\n",
    "\n",
    "# No race condition\n",
    "atomic_counter_kernel[gridsize, blocksize](counter)\n",
    "\n",
    "print(f'counter={counter.copy_to_host()[0]}, expected={n_threads}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a slightly more useful example: Computing a histogram from a an array.\n",
    "\n",
    "In addition, we'll show two convenience functions provided by `numba` for calculating the thread id and grid size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def hist_kernel(x, xmin, xmax, histogram_out):\n",
    "    \n",
    "    # Number of bins determined by histogram elements\n",
    "    nbins = histogram_out.shape[0]\n",
    "    bin_width = (xmax - xmin) / nbins\n",
    "    \n",
    "    # grid(1) provides unique thread id for a 1d grid\n",
    "    # gridsize(1) provides total size of grid (in threads), for a 1d grid\n",
    "    start = cuda.grid(1)\n",
    "    end = x.shape[0]\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(start, end, stride):\n",
    "        bin_number = math.floor((x[i] - xmin)/bin_width)\n",
    "        \n",
    "        if bin_number >= 0 and bin_number < nbins:\n",
    "            cuda.atomic.add(histogram_out, bin_number, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on 1M samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(size=10**6).astype(np.float32)\n",
    "xmin = np.float32(-4.0)\n",
    "xmax = np.float32(4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With CUDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   726,   7466,  46633, 157188, 287710, 288458, 157011,  46501,\n",
       "         7595,    644], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histogram_out = np.zeros(shape=10, dtype=np.int32)\n",
    "\n",
    "hist_kernel[64,64](x, xmin, xmax, histogram_out)\n",
    "histogram_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   726,   7466,  46633, 157188, 287710, 288458, 157011,  46501,\n",
       "         7595,    644])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(x, bins=10, range=(xmin, xmax))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare timings, this time also measuring with and without memory copy overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy:\n",
      "11 ms ± 118 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "CUDA, host arrays:\n",
      "3.38 ms ± 34.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "CUDA, device arrays:\n",
      "592 µs ± 1.16 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "print('numpy:')\n",
    "%timeit np.histogram(x, bins=10, range=(xmin, xmax))[0]\n",
    "\n",
    "print('\\nCUDA, host arrays:')\n",
    "%timeit hist_kernel[64,64](x, xmin, xmax, histogram_out)\n",
    "\n",
    "print('\\nCUDA, device arrays:')\n",
    "d_x = cuda.to_device(x)\n",
    "d_histogram_out = cuda.to_device(histogram_out)\n",
    "%timeit hist_kernel[64,64](d_x, xmin, xmax, d_histogram_out); cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it's necessary for threads to cooperate by working on the same data.\n",
    "However, cooperating though the global memory is extremely slow due to memory latencies.\n",
    "\n",
    "As we saw, CUDA provides **fast** shared memory only between threads that are in the same block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No CUDA tutorial can be complete without matrix multiplication example, so let's start there.\n",
    "First, we'll implement a straightforward matrix multiplication that does not take advantage of shared memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our initial implementation, each thread reads one row of matrix `A` and one column of matrix `B` and computes one corresponding element of the output `C`.\n",
    "\n",
    "<center><img src=\"img/matmul_noshared.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matmul_kernel(a, b, out):\n",
    "    \n",
    "    # Unique thread id on a 2d-grid\n",
    "    i, j = cuda.grid(2)\n",
    "    imax, jmax = cuda.gridsize(2)\n",
    "    \n",
    "    if i < out.shape[0] and j < out.shape[1]:\n",
    "        for k in range(b.shape[0]):\n",
    "            out[i, j] += a[i,k] * b[k,j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we'll be working with 2D kernel geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_dim=(32, 32), grid_dim=(32, 32)\n"
     ]
    }
   ],
   "source": [
    "# Base matrix dim\n",
    "N = 512\n",
    "\n",
    "# Input data \n",
    "a = np.ones((N, 2*N), dtype=np.float32) * 2\n",
    "b = np.ones((2*N, N), dtype=np.float32) * 3\n",
    "expected_out = np.matmul(a, b)\n",
    "\n",
    "# Kernel geometry\n",
    "blocksize = cuda.get_current_device().WARP_SIZE\n",
    "gridsize = (2*N + blocksize-1)//blocksize\n",
    "\n",
    "block_dim = (blocksize, blocksize)\n",
    "grid_dim = (gridsize, gridsize)\n",
    "\n",
    "print(f'block_dim={block_dim}, grid_dim={grid_dim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our simple kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6144., 6144., 6144., ..., 6144., 6144., 6144.],\n",
       "       [6144., 6144., 6144., ..., 6144., 6144., 6144.],\n",
       "       [6144., 6144., 6144., ..., 6144., 6144., 6144.],\n",
       "       ...,\n",
       "       [6144., 6144., 6144., ..., 6144., 6144., 6144.],\n",
       "       [6144., 6144., 6144., ..., 6144., 6144., 6144.],\n",
       "       [6144., 6144., 6144., ..., 6144., 6144., 6144.]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = np.zeros((N, N), dtype=np.float32)\n",
    "matmul_kernel[grid_dim, block_dim](a, b, out)\n",
    "\n",
    "# Make sure result is correct\n",
    "assert(np.allclose(out, expected_out))\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used lots of threads and a large grid!\n",
    "\n",
    "But, why is this implementation still inefficient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both A and B will be read many times from the **slow** global memory:\n",
    "- A will be read `B.shape[1]` times\n",
    "- B will be read `A.shape[0]` times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move on to a more efficient version which takes take advantage of **shared memory** to reduce\n",
    "global memory bandwidth.\n",
    "\n",
    "Recall that shared memory is on-chip memory available on each streaming multiprocessor.\n",
    "It is shared only between threads of the same block (even if other blocks are running on the same SM).\n",
    "\n",
    "Shared memory is scarce hardware resource, limited to 48kB per block. It should be used sparingly, as a way to reduce latency of global memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation,\n",
    "- Each thread block is responsible for computing one square sub-matrix `Csub` of the output `C`, of shape `(blocksize,blocksize)`.\n",
    "- Each thread within the block is responsible for computing one element of `Csub`.\n",
    "- `Csub` is the product of two rectangular matrices: `Asub` of shape `(block_size, A.shape[1])` which has the same row indices as `Csub`, and `Bsub` of shape `(B.shape[0], blocksize)` which has the same column indices as `Csub`.\n",
    "- These two rectangular matrices are divided into as many square matrices of shape `(blocksize, blocksize)` as necessary.\n",
    "- `Csub` is computed as the sum of the products of these square matrices. To compute this product:\n",
    "    - First we load two corresponding square matrices from global memory to shared memory with one thread loading one element of each matrix.\n",
    "    - Then each thread computes one element of the product using the shared memory.\n",
    "    - Each thread accumulates the result of each of these square products into a register and once done writes the result to global memory.\n",
    "\n",
    "<center><img src=\"img/matmul_shared.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def fast_matmul_kernel(a, b, out):\n",
    "    # Define arrays in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    a_sub = cuda.shared.array(shape=(blocksize, blocksize), dtype=numba.float32)\n",
    "    b_sub = cuda.shared.array(shape=(blocksize, blocksize), dtype=numba.float32)\n",
    "\n",
    "    # Global id of current thread in a 2D threadblock\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    # Index of thread within it's own block\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bpg = cuda.gridDim.x    # blocks per grid\n",
    "\n",
    "    # Bounds check\n",
    "    if x >= out.shape[0] or y >= out.shape[1]:\n",
    "        return\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of (blocksize,)-shaped vectors.\n",
    "    tmp = 0.\n",
    "    for i in range(bpg):\n",
    "        # Preload one element from A and B into shared memory\n",
    "        a_sub[tx, ty] = a[x, ty + i * blocksize]\n",
    "        b_sub[tx, ty] = b[tx + i * blocksize, y]\n",
    "\n",
    "        # Wait for all threads\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Compute inner product between vectors in the shared memory\n",
    "        for j in range(blocksize):\n",
    "            tmp += a_sub[tx, j] * b_sub[j, ty]\n",
    "\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    out[x, y] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the `cuda.syncthreads()` call do?\n",
    "\n",
    "Why do we need the first call? And why do we need to second?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This call allows us to use a synchronization mechanism called a **barrier**, between threads within the **same** threadblock.\n",
    "\n",
    "A barrier blocks each thread until all threads reach it, at which point all threads become unblocked.\n",
    "\n",
    "- The first `syncthreads()` call is needed in order to wait for the entire `a_sub` and `b_sub` matrices to fill, since each thread loads only one element.\n",
    "- The second `syncthreads()` is necessary so that a thread will not advance to the next sub-block. This will cause it to fetch new data into `a_sub` and `b_sub` while another thread might still need the old data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6144., 6144., 6144., ..., 6144., 6144., 6144.],\n",
       "       [6144., 6144., 6144., ..., 6144., 6144., 6144.],\n",
       "       [6144., 6144., 6144., ..., 6144., 6144., 6144.],\n",
       "       ...,\n",
       "       [6144., 6144., 6144., ..., 6144., 6144., 6144.],\n",
       "       [6144., 6144., 6144., ..., 6144., 6144., 6144.],\n",
       "       [6144., 6144., 6144., ..., 6144., 6144., 6144.]], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = np.zeros((N, N), dtype=np.float32)\n",
    "fast_matmul_kernel[grid_dim, block_dim](a, b, out)\n",
    "\n",
    "# Make sure result is correct\n",
    "assert(np.allclose(out, expected_out))\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can benchmark the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy matmul:\n",
      "4.85 ms ± 1.02 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "CUDA, naive implementation:\n",
      "10.7 ms ± 8.65 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "CUDA, with shared memory:\n",
      "5.58 ms ± 35.5 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "d_a = cuda.to_device(a)\n",
    "d_b = cuda.to_device(b)\n",
    "d_out = cuda.to_device(np.zeros((N, N), dtype=np.float32))\n",
    "\n",
    "print('numpy matmul:')\n",
    "%timeit np.matmul(a, b, out=out)\n",
    "\n",
    "print('\\nCUDA, naive implementation:')\n",
    "%timeit matmul_kernel[grid_dim, block_dim](d_a, d_b, d_out); cuda.synchronize()\n",
    "\n",
    "print('\\nCUDA, with shared memory:')\n",
    "%timeit fast_matmul_kernel[grid_dim, block_dim](d_a, d_b, d_out); cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CUDA provides a very powerful framework for easily writing highly scalable multithreaded code.\n",
    "- Once we have the right mental model about how it works, we can leverage the power of GPUs for performing arbitrary computation.\n",
    "- Using `numba`, we can do this directly in Python, and even iterate our GPU code interactively within a jupyter notebook.\n",
    "- As a bonus, we learned how to accelerate any numerical python function with `numba`, to squeeze out extra performance gains even without a GPU.\n",
    "\n",
    "You should experiment with these tools to speed up pre-and post training tasks such as data loading and preprocessing, augmentation, statistical analysis of model outputs and do on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In conclusion\n",
    "\n",
    "I hope you enjoyed our course and good luck with your projects! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
