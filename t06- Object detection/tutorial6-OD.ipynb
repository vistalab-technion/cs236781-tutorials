{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "\n",
    "\n",
    "# CS236781: Deep Learning\n",
    "# Tutorial 6: Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "\n",
    "* What is the Object Detection\n",
    "* Sliding Windows Approach\n",
    "* Performance Metrics\n",
    "* Region-based Convolutional Neural Networks (R-CNN) Family\n",
    "* You Only Look Once (YOLO) Family\n",
    "\n",
    "\n",
    "This tutorial is heavly based on Technion:EE046746 tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What Is Object Detection\n",
    "---\n",
    "Recall:\n",
    "\n",
    "* **Image classification** takes an image and predicts the object in an image. For example, when we build a cat-dog classifier, we take images of cat or dog and predict their class:\n",
    "<img src=\"./assets/tut_objdet_dog.jpg\" style=\"height:200x\">\n",
    "\n",
    "* **Object localization** - find the *location* of the cat or the dog. The problem of identifying the location of an object in an image is called **localization**. \n",
    "    * If the object class is not known, we have to predict both the location and the class of each object!\n",
    "<img src=\"./assets/tut_objdet_cat.png\" style=\"height:200px\">\n",
    "\n",
    "Both tasks together is called **Object Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The output of the algorithms are usually conducted from 5 outputs:\n",
    "\n",
    "* class score\n",
    "\n",
    "and 4 that determine the bounding box:\n",
    "\n",
    "   * `bounding_box_top_left_x_coordinate`\n",
    "   * `bounding_box_top_left_y_coordinate`\n",
    "   * `bounding_box_width`\n",
    "   * `bounding_box_height`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### No deep learning: Sliding Windows Approach\n",
    "\n",
    "<img src=\"./assets/tut_objdet_sliding.gif\" style=\"height:250px\">\n",
    "\n",
    "* **Problem** - how do you know the size of the window so that it always contains the image?\n",
    "<img src=\"./assets/tut_objdet_small_big_obj.PNG\" style=\"height:200px\">\n",
    "\n",
    "* As you can see that the object can be of varying sizes. \n",
    "* To solve this problem an **image pyramid** is created by scaling the image.\n",
    "    * The idea is to resize the image at multiple scales and rely on the fact that our chosen window size will completely contain the object in *one* of these resized images.\n",
    "* Most commonly, the image is downsampled (size is reduced) until a certain condition, typically a minimum size, is reached.\n",
    "* A fixed size window detector is run on each of these images.\n",
    "* It’s common to have as many as 64 levels on such pyramids. Now, all these windows are fed to a classifier to detect the object of interest.\n",
    "* This approach can be very expensive computationally, and thus **very slow**.\n",
    "<img src=\"./assets/tut_objdet_pyramid.PNG\" style=\"height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/performance-2.png\" style=\"height:50px;display:inline\"> Performance Metrics\n",
    "---\n",
    "* How can we tell if the *predicted* bounding box is good with respect to the *ground truth* (labeled) bounding box?\n",
    "* Two popular evaluation metrics are the **intersection over union (IoU)** and **Average Precision (AP)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Intersection over Union (IoU)\n",
    "---\n",
    "Also called the *Jaccard Index*, \n",
    "$$ IoU = \\frac{TP}{TP + FP+ FN} = \\frac{\\mid X \\cap Y \\mid}{\\mid X \\mid + \\mid Y \\mid - \\mid X \\cap Y \\mid} $$\n",
    "   * $X$ and $Y$ are the predicted and ground truth segmentation, respectively.\n",
    "   * TP is the true positives, FP false positives and FN false negatives.\n",
    "\n",
    "<img src=\"./assets/tut_seg_iou.png\" style=\"height:200px\">\n",
    "\n",
    "* Image Source: Wikipedia\n",
    "\n",
    "Yeild a value between 0 and 1 and The **higher** the IoU, the better the predicted location of the box for a given object.\n",
    "\n",
    "Typical threshold for detection: 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Average Precision (AP)\n",
    "---\n",
    "First, let's remember some definisions.\n",
    "\n",
    "The confusion matrix:\n",
    "\n",
    "<img src=\"./imgs/confmat.png\" style=\"height:400px\">\n",
    "\n",
    "precision and recall:\n",
    "\n",
    "<img src=\"./imgs/pr.png\" style=\"height:700px\">\n",
    "\n",
    "Precision is a measure of when \"\"your model predicts how often does it predicts correctly?\"\" It indicates how much we can rely on the model's positive predictions. \n",
    "\n",
    "Recall is a measure of \"\"has your model predicted every time that it should have predicted?\"\" It indicates any predictions that it should not have missed if the model is missing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can check what is the P and R for each threshold:\n",
    "\n",
    "<img src=\"./imgs/prgraph.png\" style=\"height:200px\">\n",
    "\n",
    "and define a AP-AUC\n",
    "\n",
    "<img src=\"./imgs/prauc.png\" style=\"height:200px\">\n",
    "\n",
    "In practice, we choose a small set of thresholds and aproximate the area as follow:\n",
    "\n",
    "<img src=\"./imgs/ap_prac.png\" style=\"height:200px\">\n",
    "\n",
    "we do that for all the instances of a class in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The **mean Average Precision (mAP)** is the mean of the Average Precisions computed over *all the classes* of the challenge.\n",
    "\n",
    "* Let $C$ be the number of classes: \n",
    "\n",
    "$$mAP = \\frac{\\sum_{c=1}^C AP(c)}{C} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Typically OD models are devided to two categories:\n",
    "* Two stage (RCNN)\n",
    "* One stage (SSD/YOLO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/nolan/64/zoom-region-mode.png\" style=\"height:50px;display:inline\"> Region-based Convolutional Neural Networks (R-CNN) Family\n",
    "---\n",
    "* The problem with combining CNNs with the sliding window approach is that CNNs are too slow and computationally very expensive. It is practically impossible to run CNNs on so many patches generated by a sliding window detector.\n",
    "* The R-CNN family of methods refers to the R-CNN, which may stand for “Regions with CNN Features” or “Region-Based Convolutional Neural Network,” developed by Ross Girshick, et al.\n",
    "* This includes the techniques **R-CNN**, **Fast R-CNN**, and **Faster-RCNN** designed and demonstrated for object localization and object recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/96/000000/region-code.png\" style=\"height:30px;display:inline\"> R-CNN\n",
    "---\n",
    "* The R-CNN was introduced in the 2014 paper by Ross Girshick, et al. from UC Berkeley titled <a href=\"https://arxiv.org/abs/1311.2524\">“Rich feature hierarchies for accurate object detection and semantic segmentation</a>.\n",
    "* It may have been one of the first large and successful application of convolutional neural networks to the problem of object localization, detection, and segmentation.\n",
    "* The approach was demonstrated on benchmark datasets, achieving then state-of-the-art results on the VOC-2012 dataset and the 200-class ILSVRC-2013 object detection dataset. We presented these datasets in the *Segmentation* tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The R-CNN model is comprised of three modules:\n",
    "    * **Module 1: Region Proposal** - generate and extract category independent region proposals, e.g. candidate bounding boxes.\n",
    "    * **Module 2: Feature Extractor** - extract feature from each candidate region, e.g. using a deep convolutional neural network.\n",
    "    * **Module 3: Classifier** - classify features as one of the known class, e.g. linear SVM classifier model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/tut_objdet_rcnn.png\" style=\"height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Module 1 -  Region Proposal\n",
    "---\n",
    "* **Selective Search** - computer vision technique that is used to propose candidate regions or bounding boxes of potential objects in the image.\n",
    "    * *Stage 1* - calculate initial regions.\n",
    "    * *Stage 2* - group regions with the highest similarity - repeat.\n",
    "    * *Stage 3* - generate a hierarchy of bounding boxes.\n",
    "* Selective search uses local cues like texture, intensity, color and/or a measure of insideness and etc to generate all the possible locations of the object.\n",
    "<img src=\"./assets/tut_objdet_selective_search.jpg\" style=\"height:200px\">\n",
    "* <a href=\"http://www.huppelen.nl/publications/selectiveSearchDraft.pdf\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Module 2 - Feature Extractor\n",
    "---\n",
    "* A feature vector of 4,096 dimensions is extracted *for each* object proposal using a **CNN**.\n",
    "* The feature extractor used by the model was the AlexNet deep CNN that won the ILSVRC-2012 image classification competition.\n",
    "* **Problem** - the input of the CNN should be a fixed size (224x224) but the size of each proposal *is different*.\n",
    "    * The size of the objects must be changed to a fixed size!\n",
    "    * **Solution** - use *warping* - anisotropically scales the object proposals (different scale in two directions).\n",
    "* The choice of the **CNN architecture** has a large effect on the detection performance (obviously...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/tut_objdet_warp.PNG\" style=\"height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Module 3 - Classifier\n",
    "---\n",
    "* The output of the CNN was a 4,096 element vector that describes the contents of the image that is fed to a linear SVM for classification, specifically one SVM is trained for each known class.\n",
    "* Why use SVM and not a *Softmax* layer? It *empirically* worked better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/tut_objdet_svm.PNG\" style=\"height:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Training R-CNNs\n",
    "---\n",
    "* **Training the CNN**:\n",
    "    * R-CNNs uses pre-trained CNNs, usually on pre-trained image-level (not object level) annotations.\n",
    "    * Adapting the CNN to detection and to the new domain (warped proposal windows), by training with SGD (Stochastic Gradient Descent).\n",
    "        * Uses proposals with $IoU > 0.5$ as positive examples (the rest are negative). Recall that this is a supervised task, and thus we have the ground-truth bounding boxes.\n",
    "* **Training the SVM**:\n",
    "    * Uses proposals with $IoU < 0.3$ as negative examples and ground truth regions as positive examples.\n",
    "        * Why is it different than the CNN threshold? It *empirically* works better and reduces *overfitting*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Detecting Objects with R-CNNs\n",
    "---\n",
    "* At test time, **non-maximum suppression (NMS)** is applied greedily given all scored regions in an image.\n",
    "* **Non-Maximum Supression (NMS)** - a technique which filters the proposals based on some threshold value (rejects proposals).\n",
    "    * *Input*: A list of Proposal boxes $B$, corresponding confidence scores $S$ and overlap threshold $N$.\n",
    "    * *Output*: A list of filtered proposals $D$.\n",
    "<img src=\"./assets/tut_objdet_nms.png\" style=\"height:200px\">\n",
    "* <a href=\"https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c\">Source and Read More</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Drawbacks of R-CNN\n",
    "---\n",
    "* Training is not end-to-end, but a *multi-stage* pipeline.\n",
    "* Trainning is computationally expensive in space and time (training a deep CNN on so many region proposals per image is very slow).\n",
    "* At test-time object-detection is slow, requiring a CNN-based feature extraction to pass on each of the candidate regions generated by the region proposal algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/tut_objdet_rcnn2.jpg\" style=\"height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/fast-forward.png\" style=\"height:40px;display:inline\"> Fast R-CNNs\n",
    "---\n",
    "* Given the great success of R-CNN, Ross Girshick, proposed an extension to address the speed issues of R-CNN in a 2015 paper titled <a href=\"https://arxiv.org/abs/1504.08083\">“Fast R-CNN”</a>.\n",
    "* Fast R-CNN is proposed as a **single-stage** model instead of a pipeline to learn and output regions and classifications *directly*.\n",
    "* Fast RCNN combined the **bounding box regression and classification** in the neural network training itself. \n",
    "    * Now the network has two heads, classification head, and bounding box regression head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The architecture of the model takes the photograph and a set of region proposals (from a selective search) as input that are passed through a deep convolutional neural network.\n",
    "* A pre-trained CNN, such as a VGG-16, is used for feature extraction. \n",
    "* The end of the deep CNN is a custom layer called a **Region of Interest Pooling** Layer, or **RoI Pooling**, that extracts features specific for a given input candidate region.\n",
    "\n",
    "<img src=\"./imgs/tut_objdet_roi1.PNG\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/tut_objdet_roi2.png\" style=\"height:250px\">\n",
    "\n",
    "* <a href=\"https://deepsense.ai/region-of-interest-pooling-in-tensorflow-example/\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The output of the CNN is then interpreted by a fully connected layer then the model has two outputs, one for the **class prediction** via a Softmax layer, and another with a linear output for the **bounding box**. \n",
    "* This process is then repeated multiple times for each region of interest in a given image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/tut_objdet_faster_rcnn.png\" style=\"height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The model is **significantly faster** to train and to make predictions, yet still requires a set of candidate regions to be proposed along with each input image.\n",
    "* Faster detection with **Truncated SVD** - compresses the fully-connected layers and reduces detection time by more than 30\\% with only a small drop in mAP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/the-flash-sign.png\" style=\"height:40px;display:inline\"> Faster R-CNNs\n",
    "---\n",
    "* The *slowest* part in Fast RCNN was **Selective Search** or Edge boxes.\n",
    "* The Fast RCNN model architecture was further improved for both speed of training and detection by Shaoqing Ren, et al. in the 2016 paper titled <a href=\"https://arxiv.org/abs/1506.01497\">“Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks”</a>.\n",
    "* The architecture was the basis for the *first-place* results achieved on both the ILSVRC-2015 and MS COCO-2015 object recognition and detection competition tasks.\n",
    "* The idea was to replace selective search with a very small convolutional network called **Region Proposal Network** to generate regions of Interests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/tut_objdet_faster_rcnn_arch.png\" style=\"height:350px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The architecture was designed to both **propose and refine** region proposals as part of the training process, referred to as a **Region Proposal Network, or RPN**. \n",
    "* These regions are then used in concert with a *Fast R-CNN* model in a single model design. \n",
    "* These improvements both reduce the number of region proposals and accelerate the test-time operation of the model to near real-time with then state-of-the-art performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The architecture is comprised of two modules:\n",
    "    * **Module 1: Region Proposal Network** - CNN for proposing regions and the type of object to consider in the region.\n",
    "    * **Module 2: Fast R-CNN** - CNN for extracting features from the proposed regions and outputting the bounding box and class labels.\n",
    "* Both modules operate on the **same output of a deep CNN**. \n",
    "* The region proposal network acts as an *attention* mechanism for the Fast R-CNN network, informing the second network of where to look or pay attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The RPN works by taking the output of a pre-trained deep CNN, such as VGG-16, and passing a small network over the feature map and outputting multiple region proposals and a class prediction for each. \n",
    "* Region proposals are *bounding boxes*, based on **anchor boxes** or pre-defined shapes designed to accelerate and improve the proposal of regions. \n",
    "* The class prediction is binary, indicating the presence of an object, or not, so-called “objectness” of the proposed region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **RPN Steps**:\n",
    "    * Generate (pre-defined) anchor boxes.\n",
    "    * Feed the possible regions into the RPN and classify each anchor box whether it is foreground or background (anchors that have a higher overlap with ground-truth boxes should be labeled as foreground, while others should be labeled as background).\n",
    "    * The output is fed into a Softmax or logistic regression activation function, to predict the labels for each anchor. A similar process is used to refine the anchors and define the bounding boxes for the selected features (learn the shape offsets for anchor boxes to fit them for objects)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/tut_objdet_rpn_1.png\" style=\"height:250px\">\n",
    "\n",
    "* <a href=\"https://towardsdatascience.com/region-proposal-network-a-detailed-view-1305c7875853\">Image Source and Read More (1)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Faster-RCNN is 10 times faster than Fast-RCNN with similar accuracy of datasets like VOC-2007. \n",
    "* That is why Faster-RCNN is one of the most accurate object detection algorithms.\n",
    "\n",
    "| |R-CNN|Fast R-CNN|Faster R-CNN |\n",
    "|--|----|----------|-------------|\n",
    "|Test Time Per Image (sec)|50|2|0.2|\n",
    "|Speed Up|1x|25x|250x|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/tut_objdet_rcnn-family-summary.png\" style=\"height:300px\">\n",
    "\n",
    "* <a href=\"https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/olympic-torch.png\" style=\"height:50px;display:inline\"> R-CNN Family Implementations\n",
    "---\n",
    "* R-CNN, Fast R-CNN, Faster R-CNN and even Mask R-CNN are all implemented in **Detectron2 for PyTorch**.\n",
    "    * It is better to use Faster R-CNN or Mask R-CNN (if you need segmentation).\n",
    "* <a href=\"https://github.com/facebookresearch/detectron2\">Detectron2 for PyTorch</a>\n",
    "    * <a href=\"https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5\">Colab Notebook Demo of Detectron2</a>\n",
    "    \n",
    "<img src=\"./assets/tut_objdet_detectron.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/cotton/64/000000/opera-glasses.png\" style=\"height:50px;display:inline\"> You Only Look Once (YOLO) Family\n",
    "---\n",
    "* All the methods discussed above handled detection as a classification problem by building a pipeline where first object proposals are generated and then these proposals are sent to classification/regression heads.\n",
    "* Another popular family of object recognition models is referred to collectively as YOLO or “You Only Look Once,” developed by Joseph Redmon, et al. which is a **regression**-based detection method.\n",
    "* The R-CNN models may be generally *more accurate*, yet the YOLO family of models are *fast*, much faster than R-CNN, achieving object detection in *real-time*.\n",
    "* <a href=\"https://pjreddie.com/darknet/yolo/\">YOLO Official Website</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  YOLO (v1)\n",
    "---\n",
    "* The YOLO model was first described by Joseph Redmon, et al. in the 2015 paper titled <a href=\"https://arxiv.org/abs/1506.02640\">“You Only Look Once: Unified, Real-Time Object Detection”</a>.\n",
    "* The approach involves a **single neural network** trained end-to-end that takes an image as input and predicts bounding boxes and class labels for each bounding box directly.\n",
    "* The technique offers lower predictive accuracy (e.g. more localization errors), although operates at 45 frames per second and up to 155 frames per second for a speed-optimized version of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The model works by first splitting the input image into a **grid of cells**, where each cell is responsible for predicting a bounding box if the center of a bounding box falls within it.\n",
    "    * YOLO divides each image into a grid of $S \\times S$ and each grid cell predicts $N$ (usually $N=2$) bounding boxes and confidence where a bounding box involving the $x, y$ coordinate, the width, the height and the confidence. \n",
    "    * The confidence reflects the accuracy of the bounding box and whether the bounding box actually contains an object (regardless of class).\n",
    "        * YOLO uses Non-Maximal Suppression (NMS) to only keep the best bounding box. The first step in NMS is to remove all the predicted bounding boxes that have a detection probability that is less than a given NMS threshold.\n",
    "    * YOLO also predicts the classification score for each box for every class in training.\n",
    "    * A total $S\\times S \\times N$ boxes are predicted. However, most of these boxes have low confidence scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/tut_objdet_yolo.png\" style=\"height:350px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/tut_objdet_yolo_arch.png\" style=\"height:250px\">\n",
    "\n",
    "* $B$ is the number of bounding boxes from each cell, $C$ is the number of classes.\n",
    "* <a href=\"https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html#yolo-you-only-look-once\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* In test time, the test image is first broken up into a grid and the network then produces output vectors, one for each grid cell. \n",
    "* These vectors tell us if a cell has an object in it, what class the object is, and the bounding boxes for the object. \n",
    "* ter producing these output vectors, non-maximal suppression is used to get rid of unlikely bounding boxes. \n",
    "    \n",
    "    \n",
    "* YOLO sees the complete image at once as opposed to looking at only a generated region proposals in the previous methods. \n",
    "* This contextual information helps in avoiding false positives. \n",
    "* However, one limitation for YOLO is that it only predicts one type of class in one grid cell. Hence, it **struggles with very small objects**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  YOLO v2 \n",
    "---\n",
    "* The model was updated by Joseph Redmon and Ali Farhadi in an effort to further improve model performance in their 2016 paper titled <a href=\"https://arxiv.org/abs/1612.08242\">\"YOLO9000: Better, Faster, Stronger\"</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Main Improvements\n",
    "---\n",
    "1. BatchNorm helps: Add batch norm on all the convolutional layers, leading to significant improvement over convergence.\n",
    "\n",
    "\n",
    "2. Image resolution matters: Fine-tuning the base model with high resolution images improves the detection performance.\n",
    "\n",
    "\n",
    "3. Convolutional anchor box detection: Rather than predicts the bounding box position with fully-connected layers over the whole feature map, YOLOv2 uses convolutional layers to predict locations of anchor boxes, like in faster R-CNN. The prediction of spatial locations and class probabilities are decoupled. Overall, the change leads to a slight decrease in mAP, but an increase in recall.\n",
    "\n",
    "\n",
    "4. K-mean clustering of box dimensions: Different from faster R-CNN that uses hand-picked sizes of anchor boxes, YOLOv2 runs k-mean clustering on the training data to find good priors on anchor box dimensions. The distance metric is designed to rely on IoU scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "5. Direct location prediction: YOLOv2 formulates the bounding box prediction in a way that it would not diverge from the center location too much. If the box location prediction can place the box in any part of the image, like in regional proposal network, the model training could become unstable.\n",
    "\n",
    "<img src=\"./assets/tut_objdet_yolov2.png\" style=\"height:350px\">\n",
    "\n",
    "6. Add fine-grained features: YOLOv2 adds a passthrough layer to bring fine-grained features from an earlier layer to the last output layer. The mechanism of this passthrough layer is similar to identity mappings in ResNet to extract higher-dimensional features from previous layers. This leads to 1% performance increase.\n",
    "\n",
    "\n",
    "7. Multi-scale training: In order to train the model to be robust to input images of different sizes, a new size of input dimension is randomly sampled every 10 batches. Since conv layers of YOLOv2 downsample the input dimension by a factor of 32, the newly sampled size is a multiple of 32.\n",
    "\n",
    "\n",
    "8. Light-weighted base model: To make prediction even faster, YOLOv2 adopts a light-weighted base model, DarkNet-19, which has 19 conv layers and 5 max-pooling layers. The key point is to insert avg poolings and 1x1 conv filters between 3x3 conv layers.\n",
    "\n",
    "\n",
    "9. Reach data: made use of Imagenet Dataset as well, used a graph distance to determine the class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## YOLO v3\n",
    "\n",
    "* Further improvements to the model were proposed by Joseph Redmon and Ali Farhadi in their 2018 paper titled <a href=\"https://arxiv.org/abs/1804.02767\">\"YOLOv3\"</a>. \n",
    "\n",
    "\n",
    "1. The objectness score uses sigmoid.\n",
    "2. Switched from multiclass classification to multilabel classification, so no softmaxes in favor of binary cross-entropy.\n",
    "3. Predictions are made for bboxes at three scales, output tensor size: N * N * (3 * (4 + 1 + num_classes))\n",
    "4. New, deeper, and more accurate backbone/feature extractor Darknet-53 (omparable to ResNet-152, but with 1.5 times fewer operations-> double the FPS on GPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/all-4.png\" style=\"height:40px;display:inline\"> YOLO v4\n",
    "---\n",
    "* In 2020, a major improvement to the YOLO model was intorduced in the paper <a href=\"https://arxiv.org/abs/2004.10934\">YOLOv4: Optimal Speed and Accuracy of Object Detection</a>.\n",
    "* YOLOv4’s architecture is composed of CSPDarknet53 (CSP: Cross-Stage-Partial connections, separating the current layer into 2 parts, one that will go through a block of convolutions, and one that won’t and then aggregate the results) as a **backbone**, spatial pyramid pooling additional module, PANet path-aggregation **neck** and **YOLOv3 head**.\n",
    "* [Bag-Of-Freebies](https://arxiv.org/pdf/1902.04103.pdf) (BoF) and Bag-Of-Specials (BoS) - improvements such as strong augementations, regularizations and special activations.\n",
    "* <a href=\"https://github.com/Tianxiaomo/pytorch-YOLOv4\">YOLOv4 PyTorch Code<a/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/tut_objdet_yolov4res.png\" style=\"height:300px\">\n",
    "\n",
    "* AP (Average Precision) and FPS (Frames Per Second) increased by 10% and 12% compared to YOLOv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "More vesrsions like PP-TOLO, YOLO-X you can read about [here](https://medium.com/deelvin-machine-learning/the-evolution-of-the-yolo-neural-networks-family-from-v1-to-v7-48dd98702a3d)\n",
    "\n",
    "As well as the shiny new [YOLO-V7](https://github.com/WongKinYiu/yolov7) that came in 2022 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other then that\n",
    "---\n",
    "\n",
    "We do not have the time to cover all the algorithms exist.\n",
    "\n",
    "If you want to dive deeper i recommand to look at:\n",
    "\n",
    "* [Retina Net](https://arxiv.org/pdf/1708.02002.pdf) - introduce the focall loss\n",
    "\n",
    "* [EfficientDet](https://arxiv.org/pdf/1911.09070.pdf) - NAS based detector\n",
    "\n",
    "* [SSD in pytorch](https://pytorch.org/hub/nvidia_deeplearningexamples_ssd/)- Single Shot MultiBox Detector model for object detection\n",
    "\n",
    "After we will see the vision transofrmers, you can also look at\n",
    "\n",
    "* [DETR](https://arxiv.org/pdf/2005.12872.pdf)\n",
    "\n",
    "* [Swin Treasformer](https://arxiv.org/pdf/2103.14030.pdf)\n",
    "\n",
    "At any time,  you can find the state of the art models in [here](https://paperswithcode.com/task/object-detection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Credits**\n",
    "\n",
    "This tutorial was written by Moshe Kimhi.<br>\n",
    "The current version is *heavly* based on tutorial of EE 046746 written by [Tal Daniel](https://taldatech.github.io)\n",
    "\n",
    "To re-use, please provide attribution and link to the original.\n",
    "\n",
    "some content from:\n",
    "\n",
    "* https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
