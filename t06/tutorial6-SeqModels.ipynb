{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "\n",
    "# CS236781: Deep Learning\n",
    "# Tutorial 6: Sequence Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "- What RNNs are and how they work\n",
    "- Implementing basic RNNs models\n",
    "- Application example: sentiment analysis of movie reviews\n",
    "- Temporal Convolution Networks as an RNN alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theory Reminders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus far, our models have been composed of fully connected (linear) layers or convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Fully connected layers\n",
    "    - Each layer $l$ operates on the output of the previous layer ($\\vec{y}_{l-1}$) and calculates,\n",
    "        $$\n",
    "        \\vec{y}_l = \\varphi\\left( \\mat{W}_l \\vec{y}_{l-1} + \\vec{b}_l \\right),~\n",
    "        \\mat{W}_l\\in\\set{R}^{n_{l}\\times n_{l-1}},~ \\vec{b}_l\\in\\set{R}^{n_l}.\n",
    "        $$\n",
    "    - FC's have completely pre-fixed input and output dimensions.\n",
    "    \n",
    "    <center><img src=\"img/mlp.png\" width=\"600\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Convolutional layers\n",
    "    - Each layer operates on an input tensor $\\vec{x}$ containing $M$ feature maps. The $k$-th feature map of the output tensor $\\vec{y}$ is:\n",
    "        $$\n",
    "        \\vec{y}^k = \\sum_{m=1}^{M} \\vec{w}^{km}\\ast\\vec{x}^m+b^k,\\ k\\in[1,K]\n",
    "        $$\n",
    "      Where $\\ast$ denotes convolution, and $K$ is the number of output feature maps.\n",
    "      \n",
    "      <center><img src=\"img/cnn_filters.png\" width=\"500\"/></center>\n",
    "      \n",
    "    - This time the weight dimensions are not dependent on the input dimensions.\n",
    "    - Weights are shared across the spatial dimensions of the input.\n",
    "    - Output dimension changes based on input dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However,\n",
    "- Models based on these types of layers lack **persistent state**. \n",
    "- The current output is not affected by **previous inputs** (or outputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How can we model a dynamical system?\n",
    "E.g., a linear system such as\n",
    "\n",
    "$$\\vec{y}_t = a_0 + a_1 \\vec{y}_{t-1}+\\dots+a_P \\vec{y}_{t-P} + b_0 \\vec{x}_t+\\dots+b_{t-Q}\\vec{x}_{t-Q}$$\n",
    "\n",
    "Many use cases and examples: text translation, sentiment analysis, scene classification in video, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An RNN layer is similar to a regular FC layer, but it has two inputs:\n",
    "- Current sample, $\\vec{x}_t \\in\\set{R}^{d_{i}}$.\n",
    "- Previous **state**, $\\vec{h}_{t-1}\\in\\set{r}^{d_{h}}$.\n",
    "\n",
    "and it produces two outputs which depend on both:\n",
    "- Current layer output, $\\vec{y}_t\\in\\set{R}^{d_o}$.\n",
    "- Current **state**, $\\vec{h}_{t}\\in\\set{r}^{d_{h}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/rnn_cell.png\" width=\"400\"/></center>\n",
    "\n",
    "Crucially,\n",
    "- The function $\\varphi(\\cdot)$ itself is not time-dependent (but is parametrized).\n",
    "- The same layer (function) is applied at successive time steps, propagating the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A basic RNN can be defined as follows.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\forall t \\geq 0:\\\\\n",
    "\\vec{h}_t &= \\varphi_h\\left( \\mat{W}_{hh} \\vec{h}_{t-1} + \\mat{W}_{xh} \\vec{x}_t + \\vec{b}_h\\right) \\\\\n",
    "\\vec{y}_t &= \\varphi_y\\left(\\mat{W}_{hy}\\vec{h}_t + \\vec{b}_y \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where,\n",
    "- $\\vec{x}_t \\in\\set{R}^{d_{i}}$ is the input at time $t$.\n",
    "- $\\vec{h}_{t-1}\\in\\set{R}^{d_{h}}$ is the **hidden state** of a fixed dimension.\n",
    "- $\\vec{y}_t\\in\\set{R}^{d_o}$ is the output at time $t$.\n",
    "- $\\mat{W}_{hh}\\in\\set{R}^{d_h\\times d_h}$, $\\mat{W}_{xh}\\in\\set{R}^{d_h\\times d_i}$, $\\mat{W}_{hy}\\in\\set{R}^{d_o\\times d_h}$, $\\vec{b}_h\\in\\set{R}^{d_h}$ and $\\vec{b}_y\\in\\set{R}^{d_o}$ are the model weights and biases.\n",
    "- $\\varphi_h$ and $\\varphi_y$ are some non-linear functions. In many cases $\\varphi_y$ is not used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modeling time-dependence\n",
    "\n",
    "If we imagine **unrolling** a single RNN layer through time,\n",
    "<center><img src=\"img/rnn_unrolled.png\" width=\"1200\" /></center>\n",
    "\n",
    "We can see how late outputs can now be influenced by early inputs, through the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RNN models are very flexible in terms of input and output meaning.\n",
    "\n",
    "Common applications include image captioning, sentiment analysis, machine translation and more. \n",
    "\n",
    "<center><img src=\"img/rnn_use_cases.jpeg\" width=\"1200\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How would **backpropagation** work, though?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/bptt.png\" width=\"1000\"></center>\n",
    "\n",
    "1. Calculated loss from each output and accumulate\n",
    "2. Calculate Gradient of loss w.r.t. each parameter at each timestep\n",
    "3. For each parameter, accumulate gradients from all timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is known as **Backpropagation through time**, or BPTT.\n",
    "\n",
    "$$\n",
    "\\pderiv{L_t}{\\mat{W}} = \\sum_{k=1}^{t}\n",
    "\\pderiv{L_t}{\\hat y_t} \\cdot\n",
    "\\pderiv{\\hat y_t}{\\vec{h}_t} \\cdot\n",
    "\\pderiv{\\vec{h}_t}{\\vec{h}_k} \\cdot\n",
    "\\pderiv{\\vec{h}_k}{\\mat{W}}\n",
    "$$\n",
    "\n",
    "But how far back do we go? What's the limiting factor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We're limited in depth by vanishing and exploding gradients controlled by the eigenvalues of $\\mat{W}$.\n",
    "\n",
    "One pragmatic solution is to limit the number of timesteps involved in the backpropagation.\n",
    "\n",
    "<center><img src=\"img/tbptt.png\" width=\"1000\"></center>\n",
    "\n",
    "This is known as **Truncated backpropagation through time**, or TBPTT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-layered (deep) RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RNNs layers can be stacked to build a deep RNN model.\n",
    "\n",
    "<center><img src=\"img/rnn_layered.png\" width=\"1200\"/></center>\n",
    "\n",
    "- As with MLPs, adding depth allows us to model intricate hierarchical features.\n",
    "- However, now we also have a time dimension which makes the representation time-dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Based on the above equations, let's create a simple RNN layer  with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, out_dim, phi_h=torch.tanh, phi_y=torch.sigmoid):\n",
    "        super().__init__()\n",
    "        self.phi_h, self.phi_y = phi_h, phi_y\n",
    "        \n",
    "        self.fc_xh = nn.Linear(in_dim, h_dim, bias=False)\n",
    "        self.fc_hh = nn.Linear(h_dim, h_dim, bias=True)\n",
    "        self.fc_hy = nn.Linear(h_dim, out_dim, bias=True)\n",
    "        \n",
    "    def forward(self, xt, h_prev=None):\n",
    "        if h_prev is None:\n",
    "            h_prev = torch.zeros(xt.shape[0], self.fc_hh.in_features)\n",
    "        \n",
    "        ht = self.phi_h(self.fc_xh(xt) + self.fc_hh(h_prev))\n",
    "        \n",
    "        yt = self.fc_hy(ht)\n",
    "        \n",
    "        if self.phi_y is not None:\n",
    "            yt = self.phi_y(yt)\n",
    "        \n",
    "        return yt, ht\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We'll instantiate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLayer(\n",
       "  (fc_xh): Linear(in_features=1024, out_features=10, bias=False)\n",
       "  (fc_hh): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (fc_hy): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 3 # batch size\n",
    "in_dim, h_dim, out_dim = 1024, 10, 1\n",
    "\n",
    "rnn = RNNLayer(in_dim, h_dim, out_dim)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And manually \"run\" a few time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 ((3, 1)):\n",
      "tensor([[0.6092],\n",
      "        [0.4527],\n",
      "        [0.5641]], grad_fn=<SigmoidBackward>)\n",
      "h1 ((3, 10)):\n",
      "tensor([[-0.3253,  0.1187,  0.4649,  0.7870, -0.6650, -0.5900, -0.3890, -0.2207,\n",
      "         -0.0210, -0.1011],\n",
      "        [ 0.1354, -0.4635, -0.2131,  0.5795,  0.5590,  0.0958,  0.4714,  0.6022,\n",
      "          0.6905,  0.8131],\n",
      "        [-0.1787, -0.5957, -0.1554,  0.6158,  0.3674,  0.7465, -0.1909, -0.6615,\n",
      "         -0.2385,  0.1083]], grad_fn=<TanhBackward>)\n",
      "\n",
      "y2 ((3, 1)):\n",
      "tensor([[0.5789],\n",
      "        [0.5941],\n",
      "        [0.4519]], grad_fn=<SigmoidBackward>)\n",
      "h2 ((3, 10)):\n",
      "tensor([[ 0.5649,  0.6421, -0.6361,  0.6028,  0.2501, -0.1958, -0.7198, -0.0052,\n",
      "          0.6093, -0.9005],\n",
      "        [ 0.2390, -0.7378, -0.0780,  0.9072, -0.4775,  0.6405,  0.5359, -0.2134,\n",
      "          0.5885, -0.5422],\n",
      "        [ 0.1098,  0.5624,  0.0769,  0.2086,  0.7552,  0.2479, -0.2085, -0.2261,\n",
      "         -0.0115,  0.4094]], grad_fn=<TanhBackward>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# t=1\n",
    "x1 = torch.randn(N, in_dim, requires_grad=True) # requiring grad just for torchviz\n",
    "y1, h1 = rnn(x1)\n",
    "print(f'y1 ({tuple(y1.shape)}):\\n{y1}')\n",
    "print(f'h1 ({tuple(h1.shape)}):\\n{h1}\\n')\n",
    "\n",
    "# t=2\n",
    "x2 = torch.randn(N, in_dim, requires_grad=True)\n",
    "y2, h2 = rnn(x2, h1)\n",
    "print(f'y2 ({tuple(y2.shape)}):\\n{y2}')\n",
    "print(f'h2 ({tuple(h2.shape)}):\\n{h2}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As usual, let's visualize the computation graph and see what happened when we used the same RNN block twice, by looking at the graph from both $y_1$ and $y_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.42.3 (20191010.1750)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"350pt\" height=\"412pt\"\n",
       " viewBox=\"0.00 0.00 349.99 412.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 408)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-408 345.99,-408 345.99,4 -4,4\"/>\n",
       "<!-- 140550452362832 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140550452362832</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"220.32,-20 114.68,-20 114.68,0 220.32,0 220.32,-20\"/>\n",
       "<text text-anchor=\"middle\" x=\"167.5\" y=\"-6.4\" font-family=\"Times,serif\" font-size=\"12.00\">SigmoidBackward</text>\n",
       "</g>\n",
       "<!-- 140550452362640 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140550452362640</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"219.47,-76 115.52,-76 115.52,-56 219.47,-56 219.47,-76\"/>\n",
       "<text text-anchor=\"middle\" x=\"167.5\" y=\"-62.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 140550452362640&#45;&gt;140550452362832 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140550452362640&#45;&gt;140550452362832</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M167.5,-55.59C167.5,-48.7 167.5,-39.1 167.5,-30.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"171,-30.3 167.5,-20.3 164,-30.3 171,-30.3\"/>\n",
       "</g>\n",
       "<!-- 140550452512704 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140550452512704</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"104.87,-144 40.12,-144 40.12,-112 104.87,-112 104.87,-144\"/>\n",
       "<text text-anchor=\"middle\" x=\"72.5\" y=\"-130.4\" font-family=\"Times,serif\" font-size=\"12.00\">fc_hy.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"72.5\" y=\"-118.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 140550452512704&#45;&gt;140550452362640 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140550452512704&#45;&gt;140550452362640</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M96.47,-111.86C111.08,-102.63 129.66,-90.9 144.15,-81.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"146.42,-84.45 153,-76.15 142.68,-78.53 146.42,-84.45\"/>\n",
       "</g>\n",
       "<!-- 140550452512752 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140550452512752</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"211.96,-138 123.03,-138 123.03,-118 211.96,-118 211.96,-138\"/>\n",
       "<text text-anchor=\"middle\" x=\"167.5\" y=\"-124.4\" font-family=\"Times,serif\" font-size=\"12.00\">TanhBackward</text>\n",
       "</g>\n",
       "<!-- 140550452512752&#45;&gt;140550452362640 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140550452512752&#45;&gt;140550452362640</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M167.5,-117.89C167.5,-109.52 167.5,-96.84 167.5,-86.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"171,-86.2 167.5,-76.2 164,-86.2 171,-86.2\"/>\n",
       "</g>\n",
       "<!-- 140550452513136 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>140550452513136</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"211.31,-206 119.69,-206 119.69,-186 211.31,-186 211.31,-206\"/>\n",
       "<text text-anchor=\"middle\" x=\"165.5\" y=\"-192.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 140550452513136&#45;&gt;140550452512752 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>140550452513136&#45;&gt;140550452512752</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M165.77,-185.82C166.07,-176.17 166.54,-160.69 166.91,-148.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"170.42,-148.2 167.22,-138.1 163.42,-147.99 170.42,-148.2\"/>\n",
       "</g>\n",
       "<!-- 140550452513184 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>140550452513184</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151.65,-268 67.35,-268 67.35,-248 151.65,-248 151.65,-268\"/>\n",
       "<text text-anchor=\"middle\" x=\"109.5\" y=\"-254.4\" font-family=\"Times,serif\" font-size=\"12.00\">MmBackward</text>\n",
       "</g>\n",
       "<!-- 140550452513184&#45;&gt;140550452513136 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>140550452513184&#45;&gt;140550452513136</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M118,-247.89C126.47,-238.81 139.67,-224.68 150,-213.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"152.65,-215.9 156.91,-206.2 147.53,-211.12 152.65,-215.9\"/>\n",
       "</g>\n",
       "<!-- 140550452512992 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>140550452512992</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"62.99,-336 0,-336 0,-304 62.99,-304 62.99,-336\"/>\n",
       "<text text-anchor=\"middle\" x=\"31.5\" y=\"-322.4\" font-family=\"Times,serif\" font-size=\"12.00\">x1</text>\n",
       "<text text-anchor=\"middle\" x=\"31.5\" y=\"-310.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (3, 1024)</text>\n",
       "</g>\n",
       "<!-- 140550452512992&#45;&gt;140550452513184 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>140550452512992&#45;&gt;140550452513184</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M51.18,-303.86C62.95,-294.81 77.85,-283.34 89.65,-274.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"91.8,-277.02 97.6,-268.15 87.54,-271.48 91.8,-277.02\"/>\n",
       "</g>\n",
       "<!-- 140550452512944 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>140550452512944</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"152.47,-330 80.52,-330 80.52,-310 152.47,-310 152.47,-330\"/>\n",
       "<text text-anchor=\"middle\" x=\"116.5\" y=\"-316.4\" font-family=\"Times,serif\" font-size=\"12.00\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140550452512944&#45;&gt;140550452513184 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>140550452512944&#45;&gt;140550452513184</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M115.43,-309.89C114.46,-301.52 112.98,-288.84 111.74,-278.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"115.2,-277.73 110.57,-268.2 108.25,-278.54 115.2,-277.73\"/>\n",
       "</g>\n",
       "<!-- 140550452513328 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>140550452513328</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"155.98,-404 77.01,-404 77.01,-372 155.98,-372 155.98,-404\"/>\n",
       "<text text-anchor=\"middle\" x=\"116.5\" y=\"-390.4\" font-family=\"Times,serif\" font-size=\"12.00\">fc_xh.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"116.5\" y=\"-378.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (10, 1024)</text>\n",
       "</g>\n",
       "<!-- 140550452513328&#45;&gt;140550452512944 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>140550452513328&#45;&gt;140550452512944</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M116.5,-371.69C116.5,-362.4 116.5,-350.44 116.5,-340.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"120,-340.32 116.5,-330.32 113,-340.32 120,-340.32\"/>\n",
       "</g>\n",
       "<!-- 140550452513232 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>140550452513232</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"273.47,-268 169.52,-268 169.52,-248 273.47,-248 273.47,-268\"/>\n",
       "<text text-anchor=\"middle\" x=\"221.5\" y=\"-254.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 140550452513232&#45;&gt;140550452513136 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>140550452513232&#45;&gt;140550452513136</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M212.99,-247.89C204.52,-238.81 191.33,-224.68 180.99,-213.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"183.46,-211.12 174.08,-206.2 178.35,-215.9 183.46,-211.12\"/>\n",
       "</g>\n",
       "<!-- 140550452513280 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>140550452513280</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"248.15,-336 182.84,-336 182.84,-304 248.15,-304 248.15,-336\"/>\n",
       "<text text-anchor=\"middle\" x=\"215.5\" y=\"-322.4\" font-family=\"Times,serif\" font-size=\"12.00\">fc_hh.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"215.5\" y=\"-310.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (10)</text>\n",
       "</g>\n",
       "<!-- 140550452513280&#45;&gt;140550452513232 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>140550452513280&#45;&gt;140550452513232</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M217.01,-303.86C217.78,-296.13 218.73,-286.63 219.56,-278.37\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"223.07,-278.45 220.58,-268.15 216.1,-277.76 223.07,-278.45\"/>\n",
       "</g>\n",
       "<!-- 140550452513376 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>140550452513376</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"338.47,-330 266.52,-330 266.52,-310 338.47,-310 338.47,-330\"/>\n",
       "<text text-anchor=\"middle\" x=\"302.5\" y=\"-316.4\" font-family=\"Times,serif\" font-size=\"12.00\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140550452513376&#45;&gt;140550452513232 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>140550452513376&#45;&gt;140550452513232</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M290.19,-309.89C277.39,-300.41 257.13,-285.4 241.92,-274.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"243.77,-271.14 233.65,-268 239.6,-276.77 243.77,-271.14\"/>\n",
       "</g>\n",
       "<!-- 140550452513472 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>140550452513472</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"341.98,-404 263.01,-404 263.01,-372 341.98,-372 341.98,-404\"/>\n",
       "<text text-anchor=\"middle\" x=\"302.5\" y=\"-390.4\" font-family=\"Times,serif\" font-size=\"12.00\">fc_hh.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"302.5\" y=\"-378.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (10, 10)</text>\n",
       "</g>\n",
       "<!-- 140550452513472&#45;&gt;140550452513376 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>140550452513472&#45;&gt;140550452513376</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M302.5,-371.69C302.5,-362.4 302.5,-350.44 302.5,-340.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"306,-340.32 302.5,-330.32 299,-340.32 306,-340.32\"/>\n",
       "</g>\n",
       "<!-- 140550452512800 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>140550452512800</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"303.47,-138 231.52,-138 231.52,-118 303.47,-118 303.47,-138\"/>\n",
       "<text text-anchor=\"middle\" x=\"267.5\" y=\"-124.4\" font-family=\"Times,serif\" font-size=\"12.00\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140550452512800&#45;&gt;140550452362640 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>140550452512800&#45;&gt;140550452362640</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M252.31,-117.89C236.06,-108.14 210.08,-92.55 191.15,-81.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"192.87,-78.15 182.5,-76 189.27,-84.15 192.87,-78.15\"/>\n",
       "</g>\n",
       "<!-- 140550452513088 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>140550452513088</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"307.7,-212 229.29,-212 229.29,-180 307.7,-180 307.7,-212\"/>\n",
       "<text text-anchor=\"middle\" x=\"268.5\" y=\"-198.4\" font-family=\"Times,serif\" font-size=\"12.00\">fc_hy.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"268.5\" y=\"-186.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (1, 10)</text>\n",
       "</g>\n",
       "<!-- 140550452513088&#45;&gt;140550452512800 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>140550452513088&#45;&gt;140550452512800</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M268.26,-179.69C268.12,-170.4 267.94,-158.44 267.79,-148.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"271.29,-148.26 267.64,-138.32 264.29,-148.37 271.29,-148.26\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fd473c93fd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchviz\n",
    "\n",
    "torchviz.make_dot(\n",
    "    y1, # Note: Change here to y1 to see the fullly unrolled graph!\n",
    "    params=dict(list(rnn.named_parameters()) + [('x1', x1), ('x2', x2)])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1: Sentiment analysis for movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The task: Given a review about a movie written by some user, decide whether it's **positive**, **negative** or **neutral**.\n",
    "\n",
    "<center><img src=\"img/sentiment_analysis.png\" width=\"500\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Classically this is considered a challenging task if approached based on keywords alone.\n",
    "\n",
    "Consider:\n",
    "\n",
    "     \"This movie was actually neither that funny, nor super witty.\"\n",
    "     \n",
    "To comprehend such a sentence, it's intuitive to see that some \"state\" must be kept when \"reading\" it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataset\n",
    "\n",
    "We'll use the [`torchtext`](https://github.com/pytorch/text) package, which provides useful tools for working ith textual data, and also includes some built-in datasets and dataloaders (similar to `torchvision`).\n",
    "\n",
    "Out dataset will be the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/treebank.html) (SST) dataset, which contains ~10,000 **labeled** movie reviews.\n",
    "\n",
    "The label of each review is either \"positive\", \"neutral\" or \"negative\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Loading and tokenizing text samples\n",
    "\n",
    "The `torchtext.data.Field` class takes care of splitting text into unique \"tokens\"\n",
    "(~words) and converting it a numerical representation as a sequence of numbers representing\n",
    "the tokens in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torchtext.data\n",
    "\n",
    "# torchtext Field objects parse text (e.g. a review) and create a tensor representation\n",
    "\n",
    "# This Field object will be used for tokenizing the movie reviews text\n",
    "review_parser = torchtext.data.Field(\n",
    "    sequential=True, use_vocab=True, lower=True,\n",
    "    init_token='<sos>', eos_token='<eos>', dtype=torch.long,\n",
    "    tokenize='spacy', tokenizer_language='en_core_web_sm'\n",
    ")\n",
    "\n",
    "# This Field object converts the text labels into numeric values (0,1,2)\n",
    "label_parser = torchtext.data.Field(\n",
    "    is_target=True, sequential=False, unk_token=None, use_vocab=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 8544\n",
      "Number of test     samples: 2210\n"
     ]
    }
   ],
   "source": [
    "import torchtext.datasets\n",
    "\n",
    "# Load SST, tokenize the samples and labels\n",
    "# ds_X are Dataset objects which will use the parsers to return tensors\n",
    "ds_train, ds_valid, ds_test = torchtext.datasets.SST.splits(\n",
    "    review_parser, label_parser, root=data_dir\n",
    ")\n",
    "\n",
    "n_train = len(ds_train)\n",
    "print(f'Number of training samples: {n_train}')\n",
    "print(f'Number of test     samples: {len(ds_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lets print some examples from our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample#0111 [positive]:\n",
      " > the film aims to be funny , uplifting and moving , sometimes all at once .\n",
      "\n",
      "sample#4321 [neutral ]:\n",
      " > the most anti - human big studio picture since 3000 miles to graceland .\n",
      "\n",
      "sample#7777 [negative]:\n",
      " > an ugly , revolting movie .\n",
      "\n",
      "sample#0000 [positive]:\n",
      " > the rock is destined to be the 21st century 's new ` ` conan '' and that he 's going to make a splash even greater than arnold schwarzenegger , jean - claud van damme or steven segal .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in ([111, 4321, 7777, 0]):\n",
    "    example = ds_train[i]\n",
    "    label = example.label\n",
    "    review = str.join(\" \", example.text)\n",
    "    print(f'sample#{i:04d} [{label:8s}]:\\n > {review}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Building a vocabulary\n",
    "\n",
    "The `Field` object can build a **vocabulary** for us,\n",
    "which is simply a bi-directional mapping between a unique index and a token.\n",
    "\n",
    "We'll only include words from the training set in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in training samples: 15482\n",
      "Number of tokens in training labels: 3\n"
     ]
    }
   ],
   "source": [
    "review_parser.build_vocab(ds_train)\n",
    "label_parser.build_vocab(ds_train)\n",
    "\n",
    "print(f\"Number of tokens in training samples: {len(review_parser.vocab)}\")\n",
    "print(f\"Number of tokens in training labels: {len(label_parser.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 20 tokens:\n",
      " ['<unk>', '<pad>', '<sos>', '<eos>', '.', 'the', ',', 'a', 'and', 'of', 'to', '-', 'is', \"'s\", 'it', 'that', 'in', 'as', 'but', 'film']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'first 20 tokens:\\n', review_parser.vocab.itos[:20], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note the **special tokens**, `<unk>`, `<pad>`, `<sos>` and `<eos>` at indexes `0-3`.\n",
    "These were automatically created by the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word=film            index=19\n",
      "word=actor           index=492\n",
      "word=schwarzenegger  index=3404\n",
      "word=spielberg       index=715\n"
     ]
    }
   ],
   "source": [
    "# Show that some words exist in the vocab\n",
    "for w in ['film', 'actor', 'schwarzenegger', 'spielberg']:\n",
    "    print(f'word={w:15s} index={review_parser.vocab.stoi[w]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels vocab:\n",
      " {'positive': 0, 'negative': 1, 'neutral': 2}\n"
     ]
    }
   ],
   "source": [
    "print(f'labels vocab:\\n', dict(label_parser.vocab.stoi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Data loaders (iterators)\n",
    "\n",
    "The `torchtext` package comes with `Iterator`s, similar to the `DataLoaders` we previously worked with.\n",
    "\n",
    "A key issue when working with text sequences is that each sample is of a different length.\n",
    "\n",
    "So, how can we work with **batches** of data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "# BucketIterator creates batches with samples of similar length\n",
    "# to minimize the number of <pad> tokens in the batch.\n",
    "dl_train, dl_valid, dl_test = torchtext.data.BucketIterator.splits(\n",
    "    (ds_train, ds_valid, ds_test), batch_size=BATCH_SIZE,\n",
    "    shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets look at a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = \n",
      " tensor([[    2,     2,     2,     2],\n",
      "        [    5,  2510, 14117,    14],\n",
      "        [ 1677,   378,    67,    13],\n",
      "        [  742,    28,    66,    32],\n",
      "        [  137,  1306,   852,   349],\n",
      "        [   31,   930,  7505,    17],\n",
      "        [  275,    21,    10,   429],\n",
      "        [    5,   484,    25,    51],\n",
      "        [   22,  4637,  1353,   356],\n",
      "        [   43,     6,     6,    17],\n",
      "        [  155,    18,  8051,    25],\n",
      "        [ 4154,    14,    55,  1557],\n",
      "        [    5,    95,  4313,     6],\n",
      "        [  289,  1353, 14288,    18],\n",
      "        [ 4890,   654,    39,    57],\n",
      "        [   14,     7,    27,    40],\n",
      "        [ 1890,   179,   782,    97],\n",
      "        [ 3751,  7764,    35,   252],\n",
      "        [    4,   419,     8,   930],\n",
      "        [    3,    41,    27,    10],\n",
      "        [    1,     7,  1151,   275],\n",
      "        [    1,  1729,    35,    23],\n",
      "        [    1,     9,     8,    43],\n",
      "        [    1,    25,    44,   140],\n",
      "        [    1,  6441,    15,     7],\n",
      "        [    1,     4, 12122,  1026],\n",
      "        [    1,     3,     6,  1081],\n",
      "        [    1,     1,    18,     9],\n",
      "        [    1,     1,   584,    73],\n",
      "        [    1,     1,    61,     4],\n",
      "        [    1,     1,    24,     3],\n",
      "        [    1,     1,  3825,     1],\n",
      "        [    1,     1,   453,     1],\n",
      "        [    1,     1,    49,     1],\n",
      "        [    1,     1,  5832,     1],\n",
      "        [    1,     1,  1081,     1],\n",
      "        [    1,     1,   524,     1],\n",
      "        [    1,     1,    35,     1],\n",
      "        [    1,     1,    73,     1],\n",
      "        [    1,     1,    20,     1],\n",
      "        [    1,     1,     7,     1],\n",
      "        [    1,     1, 10987,     1],\n",
      "        [    1,     1,    39,     1],\n",
      "        [    1,     1,    23,     1],\n",
      "        [    1,     1,     4,     1],\n",
      "        [    1,     1,     3,     1]]) torch.Size([46, 4])\n",
      "\n",
      "y = \n",
      " tensor([1, 0, 1, 0]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dl_train))\n",
    "\n",
    "X, y = batch.text, batch.label\n",
    "print('X = \\n', X, X.shape, end='\\n\\n')\n",
    "print('y = \\n', y, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What are we looking at?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our sample tensor `X` is of shape `(sentence_length, batch_size)`.\n",
    "\n",
    "Note that:\n",
    "1. `sentence_length` changes every batch! You can re-run the previous block to see this.\n",
    "2. Sequence dimension first (not batch). When we implement the model, you'll see why it's easier to work this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model\n",
    "\n",
    "We'll now create our sentiment analysis model based on the simple `RNNLayer` we've implemented above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model will:\n",
    "- Take an input batch of tokenized sentences.\n",
    "- Compute a dense word-embedding of each token.\n",
    "- Process the sentence **sequentially** through the RNN layer.\n",
    "- Produce a `(B, 3)` tensor, which we'll interpret as class probabilities for each sentence in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is a **word embedding**? How do we get one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Embeddings encode tokens as tensors in a way that maintain some **semantic** meaning for our task.\n",
    "\n",
    "<center><img src=\"img/word_embeddings.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Generally we should take a pre-trained embedding depending on our task type.\n",
    "\n",
    "Here we'll train an Embedding together with our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 3, 3, 3, 3, 0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4597,  0.2945, -1.1264, -1.3419,  0.4035,  0.9961,  0.2710,  1.6505],\n",
       "        [-2.1591, -1.4564,  1.7908,  0.0338,  0.0664,  0.3454, -1.2780,  0.7022],\n",
       "        [-2.1591, -1.4564,  1.7908,  0.0338,  0.0664,  0.3454, -1.2780,  0.7022],\n",
       "        [-2.1591, -1.4564,  1.7908,  0.0338,  0.0664,  0.3454, -1.2780,  0.7022],\n",
       "        [-2.1591, -1.4564,  1.7908,  0.0338,  0.0664,  0.3454, -1.2780,  0.7022],\n",
       "        [ 0.6942,  2.5393, -1.0735, -1.1092,  0.3154,  1.8286, -2.2860,  0.2483]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = nn.Embedding(num_embeddings=5, embedding_dim=8)\n",
    "\n",
    "token_idx = torch.randint(low=0, high=5, size=(6,))\n",
    "print(token_idx)\n",
    "embedding_layer(token_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "OK, model time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_dim, embedding_dim, h_dim, out_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # nn.Embedding converts from token index to dense tensor\n",
    "        self.embedding = nn.Embedding(vocab_dim, embedding_dim)\n",
    "        \n",
    "        # Our own Vanilla RNN layer, without phi_y so it outputs a class score\n",
    "        self.rnn = RNNLayer(in_dim=embedding_dim, h_dim=h_dim, out_dim=out_dim, phi_y=None)\n",
    "        \n",
    "        # To convert class scores to log-probability we'll apply log-softmax\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # X shape: (S, B) Note batch dim is not first!\n",
    "        \n",
    "        embedded = self.embedding(X) # embedded shape: (S, B, E)\n",
    "        \n",
    "        # Loop over (batch of) tokens in the sentence(s)\n",
    "        ht = None\n",
    "        for xt in embedded:           # xt is (B, E)\n",
    "            yt, ht = self.rnn(xt, ht) # yt is (B, D_out)\n",
    "        \n",
    "        # Class scores to log-probability\n",
    "        yt_log_proba = self.log_softmax(yt)\n",
    "        \n",
    "        return yt_log_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's instantiate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(15482, 100)\n",
       "  (rnn): RNNLayer(\n",
       "    (fc_xh): Linear(in_features=100, out_features=128, bias=False)\n",
       "    (fc_hh): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (fc_hy): Linear(in_features=128, out_features=3, bias=True)\n",
       "  )\n",
       "  (log_softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = len(review_parser.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 3\n",
    "\n",
    "model = SentimentRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Test a manual forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model(X) = \n",
      " tensor([[-1.2143, -1.0999, -0.9938],\n",
      "        [-1.2143, -1.0999, -0.9938],\n",
      "        [-1.1641, -1.3582, -0.8424],\n",
      "        [-1.2143, -1.0999, -0.9938]], grad_fn=<LogSoftmaxBackward>) torch.Size([4, 3])\n",
      "labels =  tensor([1, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "print(f'model(X) = \\n', model(X), model(X).shape)\n",
    "print(f'labels = ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How big is our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RNN model has 1,577,899 trainable weights.\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The RNN model has {count_parameters(model):,} trainable weights.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why so many? We used only one RNN layer.\n",
    "\n",
    "Where are most of the weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training\n",
    "\n",
    "Let's complete the example by showing the regular pytorch-style train loop with this model.\n",
    "\n",
    "We'll run only a few epochs on a small subset just to test that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, dataloader, max_epochs=100, max_batches=200):\n",
    "    for epoch_idx in range(max_epochs):\n",
    "        total_loss, num_correct = 0, 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            X, y = batch.text, batch.label\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred_log_proba = model(X)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(y_pred_log_proba, y)\n",
    "            loss.backward()\n",
    "\n",
    "            # Weight updates\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            total_loss += loss.item()\n",
    "            y_pred = torch.argmax(y_pred_log_proba, dim=1)\n",
    "            num_correct += torch.sum(y_pred == y).float().item()\n",
    "\n",
    "            if batch_idx == max_batches-1:\n",
    "                break\n",
    "                \n",
    "        print(f\"Epoch #{epoch_idx}, loss={total_loss /(max_batches):.3f}, accuracy={num_correct /(max_batches*BATCH_SIZE):.3f}, elapsed={time.time()-start_time:.1f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0, loss=1.099, accuracy=0.414, elapsed=3.2 sec\n",
      "Epoch #1, loss=1.064, accuracy=0.420, elapsed=3.2 sec\n",
      "Epoch #2, loss=1.074, accuracy=0.398, elapsed=3.5 sec\n",
      "Epoch #3, loss=1.099, accuracy=0.374, elapsed=3.8 sec\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "rnn_model = SentimentRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
    "\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Recall: LogSoftmax + NLL is equiv to CrossEntropy on the class scores\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "train(rnn_model, optimizer, loss_fn, dl_train, max_epochs=4) # just a demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Limitations\n",
    "\n",
    "As usual this is a very nave model, just for demonstration.\n",
    "It lacks many tricks of the NLP trade, such was pre-trained embeddings,\n",
    "gated RNN units, deep or bi-directional models, dropout, etc.\n",
    "\n",
    "Don't expect SotA results :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2: Temporal Convolution Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "RNNs are sometimes useful but they have some serious drawbacks:\n",
    "\n",
    "1. Vanilla RNNs are very hard to train on long sequences (large `S`) and thus are almost never used.\n",
    "Instead, more complex RNNs like LSTMs and GRUs are employed (as you'll see in HW3 :).\n",
    "\n",
    "2. They require processing the input **sequentially** which results in poor performance for both training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But CNNs are great! Can we use them instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "OK, but how do we deal with:\n",
    "1. Input and output must both be sequences of same length, which should be arbitrary.\n",
    "1. Output at time $t$ only depends on inputs up to time $t$.\n",
    "1. Long-term dependencies across time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Input and output must both be sequences of same length, which should be arbitrary.<br>\n",
    "   $\\Longrightarrow$ 1D fully-convolutional architecture with appropriate **padding**.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Output at time $t$ only depends on inputs up to time $t$.<br>\n",
    "    $\\Longrightarrow$ **Causal convolutions**, where an output at time $t$ is convolved only with elements from time $t$ and earlier in the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Long-term dependencies across time.<br>\n",
    "    $\\Longrightarrow$ Increase receptive field using **depth** (facilitated by residual connections) and **dilated convolutions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Combining these techniques gives us the TCN architecture (Bai et. al, 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/tcn.png\" width=\"1400\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's implement a first a **general** TCN residual block and then a full TCN model for our Sentiment Analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Control padding to maintain output size for a fixed kernel size\n",
    "        padding = (kernel_size - 1) * dilation\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=padding, dilation=dilation),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size, stride=1, padding=padding, dilation=dilation),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        if in_channels != out_channels:\n",
    "            self.channels_adapter = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.channels_adapter = None\n",
    "            \n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Main branch\n",
    "        out = self.conv1(x)\n",
    "        out = out[..., :-self.padding] # maintain causality\n",
    "        out = self.conv2(out)\n",
    "        out = out[..., :-self.padding]\n",
    "\n",
    "        # Skip-connection (residual branch)\n",
    "        skip = x if not self.channels_adapter else self.channels_adapter(x)\n",
    "        out = torch.relu(out + skip)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TCNBlock(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv1d(2, 4, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (channels_adapter): Conv1d(2, 4, kernel_size=(1,), stride=(1,))\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TCNBlock(in_channels=2, out_channels=4, kernel_size=3, dilation=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How does the TCN block maintain causality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/causal_convolution.png\" width=\"600\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentTCN(nn.Module):\n",
    "    def __init__(self, vocab_dim: int, embedding_dim: int, layer_channels: list, out_dim: int, kernel_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        assert len(layer_channels) > 0\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_dim, embedding_dim)\n",
    "        \n",
    "        tcn_channels = [embedding_dim] + layer_channels + [out_dim]\n",
    "        layers = []\n",
    "        for i, (c_in, c_out) in enumerate(zip(tcn_channels[:-1], tcn_channels[1:])):\n",
    "            \n",
    "            # Exponentially-increasing dilation\n",
    "            dilation = 2 ** i\n",
    "            \n",
    "            layers.append(\n",
    "                TCNBlock(c_in, c_out, kernel_size, dilation=dilation, dropout=dropout)\n",
    "            )\n",
    "            \n",
    "        self.blocks = nn.Sequential(*layers)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, **kw): # x is (S, B)\n",
    "        # First we need to embed our sequence.\n",
    "        # Note how we treat the E as channels for the convulutions.\n",
    "        x_emb = self.embedding(x) # (S, B, E)\n",
    "        x_emb = torch.transpose(x_emb, 0, 1) # (B, S, E)\n",
    "        x_emb = torch.transpose(x_emb, 1, 2) # (B, E, S)\n",
    "        \n",
    "        # Process the entire sequence (at once!)\n",
    "        y_seq = self.blocks(x_emb) # (B, D_out, S)\n",
    "        \n",
    "        # Output predictions\n",
    "        yt = y_seq[..., -1] # (B, D_out)\n",
    "        yt_log_proba = self.log_softmax(yt)\n",
    "        return yt_log_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentTCN(\n",
       "  (embedding): Embedding(15482, 100)\n",
       "  (blocks): Sequential(\n",
       "    (0): TCNBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv1d(100, 32, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (channels_adapter): Conv1d(100, 32, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (1): TCNBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): TCNBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv1d(32, 3, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv1d(3, 3, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (channels_adapter): Conv1d(32, 3, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (log_softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcn = SentimentTCN(INPUT_DIM, EMBEDDING_DIM, [32, 32], OUTPUT_DIM, kernel_size=3)\n",
    "tcn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice where the `channel_adapter`s are. What's their purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TCN model has 1,570,796 trainable weights.\n"
     ]
    }
   ],
   "source": [
    "print(f'The TCN model has {count_parameters(tcn):,} trainable weights.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try a forward pass with this new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3089, -0.7770, -1.3089],\n",
       "        [-1.3425, -0.7390, -1.3425],\n",
       "        [-1.2479, -0.8539, -1.2479],\n",
       "        [-1.3521, -0.7285, -1.3521]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcn(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And finally, let's train the new model using the exact same setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0, loss=1.078, accuracy=0.436, elapsed=2.7 sec\n",
      "Epoch #1, loss=1.078, accuracy=0.425, elapsed=2.7 sec\n",
      "Epoch #2, loss=1.071, accuracy=0.414, elapsed=2.8 sec\n",
      "Epoch #3, loss=1.065, accuracy=0.438, elapsed=2.9 sec\n"
     ]
    }
   ],
   "source": [
    "tcn_model = SentimentTCN(INPUT_DIM, EMBEDDING_DIM, [32, 32], OUTPUT_DIM, kernel_size=3)\n",
    "optimizer = optim.Adam(tcn_model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "train(tcn_model, optimizer, loss_fn, dl_train, max_epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice how:\n",
    "1. The model took sequences of different length every batch, just like an RNN.\n",
    "2. Sequences were processed in **parallel** w.r.t. time, unlike RNN, making it faster.\n",
    "3. Receptive field was controlled by architecture, not by sequence length, unlike RNN.\n",
    "4. No need for BPTT, unlike RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Thanks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Image credits**\n",
    "\n",
    "Some images in this tutorial were taken and/or adapted from:\n",
    "\n",
    "- Fundamentals of Deep Learning, Nikhil Buduma, Oreilly 2017\n",
    "- Sebastian Ruder, \"On word embeddings - Part 1\", 2016, https://ruder.io\n",
    "- Andrej Karpathy, http://karpathy.github.io\n",
    "- MIT 6.S191\n",
    "- Stanford cs231n\n",
    "- S. Bai et al. 2018, http://arxiv.org/abs/1803.01271"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
