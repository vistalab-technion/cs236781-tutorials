{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "\n",
    "# CS236605: Deep Learning\n",
    "# Tutorial 5: Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theory Reminders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far, our models have been composed of fully connected (linear) layers or convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fully connected layers\n",
    "    - Each layer $l$ operates on the output of the previous layer ($\\vec{y}_{l-1}$) and calculates,\n",
    "        $$\n",
    "        \\vec{y}_l = \\varphi\\left( \\mat{W}_l \\vec{y}_{l-1} + \\vec{b}_l \\right),~\n",
    "        \\mat{W}_l\\in\\set{R}^{n_{l}\\times n_{l-1}},~ \\vec{b}_l\\in\\set{R}^{n_l}.\n",
    "        $$\n",
    "    - FC's have completely pre-fixed input and output dimensions.\n",
    "    \n",
    "    <img src=\"img/mlp.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convolutional layers\n",
    "    - Each layer operates on an input tensor $\\vec{x}$ containing $M$ feature maps. The $k$-th feature map of the output tensor $\\vec{y}$ is:\n",
    "        $$\n",
    "        \\vec{y}^k = \\sum_{m=1}^{M} \\vec{w}^{km}\\ast\\vec{x}^m+b^k,\\ k\\in[1,K]\n",
    "        $$\n",
    "      Where $\\ast$ denotes convolution, and $K$ is the number of output feature maps.\n",
    "      \n",
    "      <img src=\"img/cnn_filters.png\" width=\"500\"/>\n",
    "    - This time the weight dimensions are not dependent on the input dimensions.\n",
    "    - Weights are shared across the spatial dimensions of the input.\n",
    "    - Output dimension changes based on input dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However,\n",
    "- Models based on these types of layers lack **persistent state**. \n",
    "- The current output is not affected by **previous inputs** (or outputs).\n",
    "\n",
    "How can we model a dynamical system?\n",
    "E.g., a linear system such as\n",
    "$$\\vec{y}_t = a_0 + a_1 \\vec{y}_{t-1}+\\dots+a_P \\vec{y}_{t-P} + b_0 \\vec{x}_t+\\dots+b_{t-Q}\\vec{x}_{t-Q}$$\n",
    "\n",
    "Many use cases and examples: text translation, sentiment classification, scene analysis in video, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An RNN layer is similar to a regular FC layer, but it has two inputs:\n",
    "- Current sample, $\\vec{x}_t \\in\\set{R}^{d_{i}}$.\n",
    "- Previous **state**, $\\vec{h}_{t-1}\\in\\set{r}^{d_{h}}$.\n",
    "\n",
    "and it produces two outputs which depend on both:\n",
    "- Current layer output, $\\vec{y}_t\\in\\set{R}^{d_o}$.\n",
    "- Current **state**, $\\vec{h}_{t}\\in\\set{r}^{d_{h}}$.\n",
    "\n",
    "<img src=\"img/rnn_cell.png\" width=\"300\"/>\n",
    "\n",
    "Crucially,\n",
    "- The function $\\varphi(\\cdot)$ itself is not time-dependent (but is parametrized).\n",
    "- The same layer (function) is applied at successive time steps, propagating the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic RNN can be defined as follows.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\forall t \\geq 0:\\\\\n",
    "\\vec{h}_t &= \\varphi_h\\left( \\mat{W}_{hh} \\vec{h}_{t-1} + \\mat{W}_{xh} \\vec{x}_t + \\vec{b}_h\\right) \\\\\n",
    "\\vec{y}_t &= \\varphi_y\\left(\\mat{W}_{hy}\\vec{h}_t + \\vec{b}_y \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where,\n",
    "- $\\vec{x}_t \\in\\set{R}^{d_{i}}$ is the input at time $t$.\n",
    "- $\\vec{h}_{t-1}\\in\\set{R}^{d_{h}}$ is the **hidden state** of a fixed dimension.\n",
    "- $\\vec{y}_t\\in\\set{R}^{d_o}$ is the output at time $t$.\n",
    "- $\\mat{W}_{hh}\\in\\set{R}^{d_h\\times d_h}$, $\\mat{W}_{xh}\\in\\set{R}^{d_h\\times d_i}$, $\\mat{W}_{hy}\\in\\set{R}^{d_o\\times d_h}$, $\\vec{b}_h\\in\\set{R}^{d_h}$ \n",
    "- $\\varphi_h$ and $\\varphi_y$ are some non-linear functions. In many cases $\\varphi_y$ is not used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $\\vec{b}_y\\in\\set{R}^{d_o}$ are the model weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling time-dependence\n",
    "\n",
    "If we imagine **unrolling** a single RNN layer through time,\n",
    "<img src=\"img/rnn_unrolled.png\" width=\"800\" />\n",
    "\n",
    "We can see how late outputs can now be influenced by early inputs, through the hidden state.\n",
    "\n",
    "How would **backpropagation** work, though?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN models are very flexible in terms of input and output meaning.\n",
    "\n",
    "Common applications include image captioning, sentiment analysis, machine translation and more. \n",
    "\n",
    "<img src=\"img/rnn_use_cases.jpeg\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layered RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs layers can be stacked to build a deep RNN model.\n",
    "\n",
    "<img src=\"img/rnn_layered.png\" width=\"800\"/>\n",
    "\n",
    "- As with MLPs, adding depth allows us to model intricate hierarchical features.\n",
    "- However, now we also have a time dimension which makes the representation time-dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above equaitions, let's create a simple layer RNN with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, out_dim, phi_h=torch.tanh, phi_y=torch.sigmoid):\n",
    "        super().__init__()\n",
    "        self.phi_h, self.phi_y = phi_h, phi_y\n",
    "        \n",
    "        self.fc_xh = nn.Linear(in_dim, h_dim, bias=False)\n",
    "        self.fc_hh = nn.Linear(h_dim, h_dim, bias=True)\n",
    "        self.fc_hy = nn.Linear(h_dim, out_dim, bias=True)\n",
    "        \n",
    "    def forward(self, xt, h_prev=None):\n",
    "        if h_prev is None:\n",
    "            h_prev = torch.zeros(xt.shape[0], self.fc_hh.in_features)\n",
    "        \n",
    "        ht = self.phi_h(self.fc_xh(xt) + self.fc_hh(h_prev))\n",
    "        \n",
    "        yt = self.fc_hy(ht)\n",
    "        \n",
    "        if self.phi_y is not None:\n",
    "            yt = self.phi_y(yt)\n",
    "        \n",
    "        return yt, ht\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLayer(\n",
       "  (fc_xh): Linear(in_features=1024, out_features=128, bias=False)\n",
       "  (fc_hh): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc_hy): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate our model\n",
    "\n",
    "N = 3 # batch size\n",
    "in_dim, h_dim, out_dim = 1024, 128, 1\n",
    "\n",
    "rnn = RNNLayer(in_dim, h_dim, out_dim)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1: tensor([[0.4339],\n",
      "        [0.4022],\n",
      "        [0.5431]], grad_fn=<SigmoidBackward>)\n",
      "y2: tensor([[0.4161],\n",
      "        [0.4258],\n",
      "        [0.4816]], grad_fn=<SigmoidBackward>)\n",
      "y3: tensor([[0.5188],\n",
      "        [0.5181],\n",
      "        [0.5371]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Manually \"run\" a few time steps\n",
    "\n",
    "# t=1\n",
    "x1 = torch.randn(N, in_dim)\n",
    "y1, h1 = rnn(x1)\n",
    "print(f'y1: {y1}')\n",
    "\n",
    "# t=2\n",
    "x2 = torch.randn(N, in_dim)\n",
    "y2, h2 = rnn(x2, h1)\n",
    "print(f'y2: {y2}')\n",
    "\n",
    "# t=3\n",
    "x3 = torch.randn(N, in_dim)\n",
    "y3, h3 = rnn(x3, h2)\n",
    "print(f'y3: {y3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1]) torch.Size([3, 128])\n"
     ]
    }
   ],
   "source": [
    "print(y3.shape, h3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application example: Sentiment analysis for movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task: Given a review about a movie written by some user, decide whether it's **positive**, **negative** or **neutral**.\n",
    "\n",
    "<img src=\"img/sentiment_analysis.png\" width=\"300\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classically this is considered a challenging task if approached based on keywords alone.\n",
    "\n",
    "Consider:\n",
    "\n",
    "     \"This movie was actually neither that funny, nor super witty.\"\n",
    "     \n",
    "To comprehend such a sentence, it's intuitive to see that some \"state\" must be kept when reading it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We'll use the [`torchtext`](https://github.com/pytorch/text) package, which provides useful tools for working ith textual data, and also includes some built-in datasets and dataloaders (similar to `torchvision`).\n",
    "\n",
    "Out dataset will be the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/treebank.html) (SST) dataset, which contains ~10,000 labeled movie reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and tokenizing text samples\n",
    "\n",
    "The `torchtext.data.Field` class takes care of splitting text into unique \"tokens\"\n",
    "(~words) and converting it a numerical representation as a sequence of numbers representing\n",
    "the tokens in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 8544\n"
     ]
    }
   ],
   "source": [
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "# This Field object will be used for tokenizing the movie reviews text\n",
    "TEXT = data.Field(tokenize='spacy', sequential=True, use_vocab=True, )\n",
    "\n",
    "# This Field object converts the labels into tokens\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "# Load SST, tokenize the samples and populate our Field objects\n",
    "# (ds_X are Dataset objects)\n",
    "ds_train, ds_valid, ds_test = datasets.SST.splits(TEXT, LABEL, root=data_dir)\n",
    "\n",
    "n_train = len(ds_train)\n",
    "print(f'Number of training samples: {n_train}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets print some examples from our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample#111: [positive] The film aims to be funny , uplifting and moving , sometimes all at once .\n",
      "sample#7777: [negative] An ugly , revolting movie .\n"
     ]
    }
   ],
   "source": [
    "for i in ([111, 7777]):\n",
    "    print(f'sample#{i}: [{ds_train[i].label}] {str.join(\" \", ds_train[i].text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a vocabulary\n",
    "\n",
    "The `Field` object can build a **vocabulary** for us,\n",
    "which is simply a bi-directional mapping between a unique index to the word.\n",
    "\n",
    "We'll only include words from the training set in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in training samples: 17200\n",
      "Number of tokens in training labels: 3\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(ds_train)\n",
    "LABEL.build_vocab(ds_train)\n",
    "\n",
    "print(f\"Number of tokens in training samples: {len(TEXT.vocab)}\")\n",
    "print(f\"Number of tokens in training labels: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 20 tokens:\n",
      " ['<unk>', '<pad>', '.', ',', 'the', 'and', 'of', 'a', 'to', '-', \"'s\", 'is', 'that', 'in', 'it', 'The', 'as', 'film', 'but', 'with']\n",
      "\n",
      "index of \"film\": 17\n"
     ]
    }
   ],
   "source": [
    "print(f'first 20 tokens:\\n', TEXT.vocab.itos[:20], end='\\n\\n')\n",
    "print(f'index of \"film\":', TEXT.vocab.stoi['film'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the **special tokens**, `<unk>` and `<pad>` at index 0 and 1. These were automatically created by the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels vocab:\n",
      " {'positive': 0, 'negative': 1, 'neutral': 2}\n"
     ]
    }
   ],
   "source": [
    "print(f'labels vocab:\\n', dict(LABEL.vocab.stoi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loaders (iterators)\n",
    "\n",
    "The `torchtext` package comes with `Iterator`s, similar to the `DataLoaders` we previously worked with.\n",
    "\n",
    "A key issue when working with text sequences is that each sample is of a different length.\n",
    "\n",
    "So, how can we work with **batches** of data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "# BucketIterator is supposed to created batches with samples of similar length\n",
    "# to minimize the number of <pad> tokens in the batch.\n",
    "dl_train, dl_valid, dl_test = data.BucketIterator.splits(\n",
    "    (ds_train, ds_valid, ds_test), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = \n",
      " tensor([[  489,   336,    23,    69],\n",
      "        [   45,     4, 15958,    11],\n",
      "        [ 3795,   912, 11368,   828],\n",
      "        [    2,  1025,    21,     3],\n",
      "        [    1,   284,  4603,   950],\n",
      "        [    1,   133,  3225,  1704],\n",
      "        [    1,    70,    12,     2],\n",
      "        [    1,   790,    92,     1],\n",
      "        [    1,     3,   197,     1],\n",
      "        [    1,    18,  5277,     1],\n",
      "        [    1,    82,     4,     1],\n",
      "        [    1,   207,  3605,     1],\n",
      "        [    1,   265,     6,     1],\n",
      "        [    1,    14,    22,     1],\n",
      "        [    1,     7,  2909,     1],\n",
      "        [    1,  6912,     2,     1],\n",
      "        [    1,     2,     1,     1]]) torch.Size([17, 4])\n",
      "\n",
      "y = \n",
      " tensor([2, 0, 1, 0]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dl_train))\n",
    "\n",
    "X, y = batch.text, batch.label\n",
    "print('X = \\n', X, X.shape, end='\\n\\n')\n",
    "print('y = \\n', y, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are we looking at?\n",
    "\n",
    "Our sample tensor `X` is of shape `(sentence_length, batch_size)`.\n",
    "\n",
    "Note that `sentence_length` changes every batch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We'll now create our sentiment analysis model based on the simple `RNNLayer` we've implemented above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will:\n",
    "- Take an input batch of tokenized sentences.\n",
    "- Compute a dense word-embedding of each token.\n",
    "- Process the sentence **sequentially** through the RNN layer.\n",
    "- Produce a `(N, 3)` tensor for each sentence which we'll interpret as class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, in_dim, embedding_dim, h_dim, out_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # nn.Embedding converts from token index to dense tensor\n",
    "        self.embedding = nn.Embedding(in_dim, embedding_dim)\n",
    "        \n",
    "        # Our custom RNN layer without phi_y outputs a class score\n",
    "        self.rnn = RNNLayer(embedding_dim, h_dim, out_dim, phi_y=None)\n",
    "        \n",
    "        # To convert class scores to log-probability we'll apply log-softmax\n",
    "        self.log_softmax = nn.LogSoftmax(dim=0)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # X shape: (S, B)\n",
    "        \n",
    "        embedded = self.embedding(X)\n",
    "        # embedded shape: (S, B, E)\n",
    "        \n",
    "        # Loop over (batch of) tokens in the sentence(s)\n",
    "        ht = None\n",
    "        for xt in embedded:\n",
    "            yt, ht = self.rnn(xt, ht)\n",
    "        \n",
    "        # Class scores to log-probability\n",
    "        yt_log_proba = self.log_softmax(yt)\n",
    "        \n",
    "        return yt_log_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, what should the `input_dim` be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(17200, 100)\n",
       "  (rnn): RNNLayer(\n",
       "    (fc_xh): Linear(in_features=100, out_features=128, bias=False)\n",
       "    (fc_hh): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (fc_hy): Linear(in_features=128, out_features=3, bias=True)\n",
       "  )\n",
       "  (log_softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 3\n",
    "\n",
    "model = SentimentRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test a manual forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model(X) = \n",
      " tensor([[-1.3864, -1.2864, -1.3838],\n",
      "        [-1.4032, -1.9383, -1.3257],\n",
      "        [-1.3695, -1.1922, -1.4561],\n",
      "        [-1.3863, -1.2865, -1.3838]], grad_fn=<LogSoftmaxBackward>) torch.Size([4, 3])\n",
      "labels =  tensor([2, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "print(f'model(X) = \\n', model(X), model(X).shape)\n",
    "print(f'labels = ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How big is our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,749,699 trainable weights.\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable weights.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why so many? We used only one RNN layer.\n",
    "\n",
    "Where are most of the weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Let's complete the example by showing the regular pytorch-style train loop with this model.\n",
    "\n",
    "We'll run only a few epochs on a small subset just to test that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = SentimentRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Recall: LogSoftmax + NLL is equiv to CrossEntropy on the class scores\n",
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0, loss=1.3987759047746657, accuracy=0.30625\n",
      "Epoch #1, loss=1.3964005160331725, accuracy=0.2925\n",
      "Epoch #2, loss=1.3910330873727799, accuracy=0.28875\n"
     ]
    }
   ],
   "source": [
    "for epoch_idx in range(3):\n",
    "    total_loss = 0\n",
    "    num_correct = 0\n",
    "    max_batches = 200\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dl_train):\n",
    "        X, y = batch.text, batch.label\n",
    "        \n",
    "        # Forward pass\n",
    "        y_pred_log_proba = model(X)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(y_pred_log_proba, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Weight updates\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        total_loss += loss.item()\n",
    "        y_pred = torch.argmax(y_pred_log_proba, dim=1)\n",
    "        num_correct += torch.sum(y_pred == y).float().item()\n",
    "        \n",
    "        if batch_idx == max_batches-1:\n",
    "            break\n",
    "    print(f\"Epoch #{epoch_idx}, loss={total_loss /(max_batches)}, accuracy={num_correct /(max_batches*BATCH_SIZE)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations\n",
    "\n",
    "As usual this is a very na√Øve model, just for demonstration.\n",
    "It lacks many tricks of the NLP trade, such was pre-trained embeddings,\n",
    "gated RNN units, deep or bi-directional models, dropout, etc.\n",
    "\n",
    "Don't expect SotA results :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, some parts of the input may be more important than others.\n",
    "\n",
    "An **Attention** mechanism, allows the model to \"focus\" on, i.e. give a *greater weight* to\n",
    "different parts of the input or some pther intermetiate part of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example from an image captioning [paper](https://arxiv.org/pdf/1502.03044.pdf) (K. Xu et al. 2015):\n",
    "\n",
    "<img src=\"img/attn_ic1.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/attn_ic2.png\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input soft attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One place to apply attention is to the **input features**.\n",
    "\n",
    "In the context of our RNN model, we can change it's hidden state update to:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\vec{a}_t &= \\sigma\\left( \\mat{W}_{ha} \\vec{h}_{t-1} + \\mat{W}_{xa} \\vec{x}_t+ \\vec{b}_a\\right) \\\\\n",
    "\\vec{g}_t &= \\mathrm{softmax}(\\alpha \\vec{a}_t) \\\\\n",
    "\\vec{h}_t &= \\varphi_h\\left( \\mat{W}_{hh} \\vec{h}_{t-1} + \\mat{W}_{xh} (\\vec{x}_t \\odot \\vec{g}_t)+ \\vec{b}_h\\right) \\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNLayerInputAttn(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, out_dim, phi_h=torch.tanh, phi_y=torch.sigmoid):\n",
    "        super().__init__()\n",
    "        self.phi_h, self.phi_y = phi_h, phi_y\n",
    "        \n",
    "        # Attention parameters\n",
    "        self.fc_xa = nn.Linear(in_dim, in_dim, bias=False)\n",
    "        self.fc_ha = nn.Linear(h_dim, in_dim, bias=True)\n",
    "        \n",
    "        # Regular RNN parameters\n",
    "        self.fc_xh = nn.Linear(in_dim, h_dim, bias=False)\n",
    "        self.fc_hh = nn.Linear(h_dim, h_dim, bias=True)\n",
    "        self.fc_hy = nn.Linear(h_dim, out_dim, bias=True)\n",
    "        \n",
    "    def forward(self, xt, h_prev=None):\n",
    "        if h_prev is None:\n",
    "            h_prev = torch.zeros(xt.shape[0], self.fc_hh.in_features)\n",
    "            \n",
    "        # Calculate the attention gating gt: a weight for each feature of x\n",
    "        at = torch.sigmoid(self.fc_xa(xt) + self.fc_ha(h_prev))\n",
    "        gt = torch.softmax(at, dim=1)\n",
    "        \n",
    "        # Apply regular RNN with gated input\n",
    "        ht = self.phi_h(self.fc_xh(xt * gt) + self.fc_hh(h_prev))\n",
    "        \n",
    "        yt = self.fc_hy(ht)\n",
    "        \n",
    "        if self.phi_y is not None:\n",
    "            yt = self.phi_y(yt)\n",
    "        \n",
    "        return yt, ht\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret this as a soft (differentiable) gating of the input.\n",
    "\n",
    "This makes sense for image captioning, where we want to emphasize image regions based on their feature maps.\n",
    "\n",
    "What about our sentiment analysis task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Image credits**\n",
    "\n",
    "Some images in this tutorial were taken and/or adapted from:\n",
    "\n",
    "- Fundamentals of Deep Learning, Nikhil Buduma, Oreilly 2017\n",
    "- Andrej Karpathy, http://karpathy.github.io\n",
    "- K. Xu et al. 2015, https://arxiv.org/abs/1502.03044"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
