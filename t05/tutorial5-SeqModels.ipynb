{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "\n",
    "# CS236781: Deep Learning\n",
    "# Tutorial 5: Sequence Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "- What RNNs are and how they work\n",
    "- Implementing basic RNNs models\n",
    "- Application example: sentiment analysis of movie reviews\n",
    "- Temporal Convolution Networks as an RNN alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theory Reminders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus far, our models have been composed of fully connected (linear) layers or convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Fully connected layers\n",
    "    - Each layer $l$ operates on the output of the previous layer ($\\vec{y}_{l-1}$) and calculates,\n",
    "        $$\n",
    "        \\vec{y}_l = \\varphi\\left( \\mat{W}_l \\vec{y}_{l-1} + \\vec{b}_l \\right),~\n",
    "        \\mat{W}_l\\in\\set{R}^{n_{l}\\times n_{l-1}},~ \\vec{b}_l\\in\\set{R}^{n_l}.\n",
    "        $$\n",
    "    - FC's have completely pre-fixed input and output dimensions.\n",
    "    \n",
    "    <center><img src=\"img/mlp.png\" width=\"600\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Convolutional layers\n",
    "    - Each layer operates on an input tensor $\\vec{x}$ containing $M$ feature maps. The $k$-th feature map of the output tensor $\\vec{y}$ is:\n",
    "        $$\n",
    "        \\vec{y}^k = \\sum_{m=1}^{M} \\vec{w}^{km}\\ast\\vec{x}^m+b^k,\\ k\\in[1,K]\n",
    "        $$\n",
    "      Where $\\ast$ denotes convolution, and $K$ is the number of output feature maps.\n",
    "      \n",
    "      <center><img src=\"img/cnn_filters.png\" width=\"500\"/></center>\n",
    "      \n",
    "    - This time the weight dimensions are not dependent on the input dimensions.\n",
    "    - Weights are shared across the spatial dimensions of the input.\n",
    "    - Output dimension changes based on input dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However,\n",
    "- Models based on these types of layers lack **persistent state**. \n",
    "- The current output is not affected by **previous inputs** (or outputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How can we model a dynamical system?\n",
    "E.g., a linear system such as\n",
    "\n",
    "$$\\vec{y}_t = a_0 + a_1 \\vec{y}_{t-1}+\\dots+a_P \\vec{y}_{t-P} + b_0 \\vec{x}_t+\\dots+b_{t-Q}\\vec{x}_{t-Q}$$\n",
    "\n",
    "Many use cases and examples: text translation, sentiment analysis, scene classification in video, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An RNN layer is similar to a regular FC layer, but it has two inputs:\n",
    "- Current sample, $\\vec{x}_t \\in\\set{R}^{d_{i}}$.\n",
    "- Previous **state**, $\\vec{h}_{t-1}\\in\\set{r}^{d_{h}}$.\n",
    "\n",
    "and it produces two outputs which depend on both:\n",
    "- Current layer output, $\\vec{y}_t\\in\\set{R}^{d_o}$.\n",
    "- Current **state**, $\\vec{h}_{t}\\in\\set{r}^{d_{h}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/rnn_cell.png\" width=\"400\"/></center>\n",
    "\n",
    "Crucially,\n",
    "- The function $\\varphi(\\cdot)$ itself is not time-dependent (but is parametrized).\n",
    "- The same layer (function) is applied at successive time steps, propagating the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A basic RNN can be defined as follows.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\forall t \\geq 0:\\\\\n",
    "\\vec{h}_t &= \\varphi_h\\left( \\mat{W}_{hh} \\vec{h}_{t-1} + \\mat{W}_{xh} \\vec{x}_t + \\vec{b}_h\\right) \\\\\n",
    "\\vec{y}_t &= \\varphi_y\\left(\\mat{W}_{hy}\\vec{h}_t + \\vec{b}_y \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where,\n",
    "- $\\vec{x}_t \\in\\set{R}^{d_{i}}$ is the input at time $t$.\n",
    "- $\\vec{h}_{t-1}\\in\\set{R}^{d_{h}}$ is the **hidden state** of a fixed dimension.\n",
    "- $\\vec{y}_t\\in\\set{R}^{d_o}$ is the output at time $t$.\n",
    "- $\\mat{W}_{hh}\\in\\set{R}^{d_h\\times d_h}$, $\\mat{W}_{xh}\\in\\set{R}^{d_h\\times d_i}$, $\\mat{W}_{hy}\\in\\set{R}^{d_o\\times d_h}$, $\\vec{b}_h\\in\\set{R}^{d_h}$ and $\\vec{b}_y\\in\\set{R}^{d_o}$ are the model weights and biases.\n",
    "- $\\varphi_h$ and $\\varphi_y$ are some non-linear functions. In many cases $\\varphi_y$ is not used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modeling time-dependence\n",
    "\n",
    "If we imagine **unrolling** a single RNN layer through time,\n",
    "<center><img src=\"img/rnn_unrolled.png\" width=\"1200\" /></center>\n",
    "\n",
    "We can see how late outputs can now be influenced by early inputs, through the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RNN models are very flexible in terms of input and output meaning.\n",
    "\n",
    "Common applications include image captioning, sentiment analysis, machine translation and more. \n",
    "\n",
    "<center><img src=\"img/rnn_use_cases.jpeg\" width=\"1200\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How would **backpropagation** work, though?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/bptt.png\" width=\"1000\"></center>\n",
    "\n",
    "1. Calculated loss from each output and accumulate\n",
    "2. Calculate Gradient of loss w.r.t. each parameter at each timestep\n",
    "3. For each parameter, accumulate gradients from all timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is known as **Backpropagation through time**, or BPTT.\n",
    "\n",
    "$$\n",
    "\\pderiv{L_t}{\\mat{W}} = \\sum_{k=1}^{t}\n",
    "\\pderiv{L_t}{\\hat y_t} \\cdot\n",
    "\\pderiv{\\hat y_t}{\\vec{h}_t} \\cdot\n",
    "\\pderiv{\\vec{h}_t}{\\vec{h}_k} \\cdot\n",
    "\\pderiv{\\vec{h}_k}{\\mat{W}}\n",
    "$$\n",
    "\n",
    "But how far back do we go? What's the limiting factor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We're limited in depth by vanishing and exploding gradients controlled by the eigenvalues of $\\mat{W}$.\n",
    "\n",
    "One pragmatic solution is to limit the number of timesteps involved in the backpropagation.\n",
    "\n",
    "<center><img src=\"img/tbptt.png\" width=\"1000\"></center>\n",
    "\n",
    "This is known as **Truncated backpropagation through time**, or TBPTT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-layered (deep) RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RNNs layers can be stacked to build a deep RNN model.\n",
    "\n",
    "<center><img src=\"img/rnn_layered.png\" width=\"1200\"/></center>\n",
    "\n",
    "- As with MLPs, adding depth allows us to model intricate hierarchical features.\n",
    "- However, now we also have a time dimension which makes the representation time-dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Based on the above equations, let's create a simple RNN layer  with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, out_dim, phi_h=torch.tanh, phi_y=torch.sigmoid):\n",
    "        super().__init__()\n",
    "        self.phi_h, self.phi_y = phi_h, phi_y\n",
    "        \n",
    "        self.fc_xh = nn.Linear(in_dim, h_dim, bias=False)\n",
    "        self.fc_hh = nn.Linear(h_dim, h_dim, bias=True)\n",
    "        self.fc_hy = nn.Linear(h_dim, out_dim, bias=True)\n",
    "        \n",
    "    def forward(self, xt, h_prev=None):\n",
    "        if h_prev is None:\n",
    "            h_prev = torch.zeros(xt.shape[0], self.fc_hh.in_features)\n",
    "        \n",
    "        ht = self.phi_h(self.fc_xh(xt) + self.fc_hh(h_prev))\n",
    "        \n",
    "        yt = self.fc_hy(ht)\n",
    "        \n",
    "        if self.phi_y is not None:\n",
    "            yt = self.phi_y(yt)\n",
    "        \n",
    "        return yt, ht\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLayer(\n",
       "  (fc_xh): Linear(in_features=1024, out_features=10, bias=False)\n",
       "  (fc_hh): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (fc_hy): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate our model\n",
    "\n",
    "N = 3 # batch size\n",
    "in_dim, h_dim, out_dim = 1024, 10, 1\n",
    "\n",
    "rnn = RNNLayer(in_dim, h_dim, out_dim)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1: tensor([[0.4863],\n",
      "        [0.4725],\n",
      "        [0.3909]], grad_fn=<SigmoidBackward>)\n",
      "h1: tensor([[ 0.9516,  0.7508, -0.0575,  0.4932,  0.6349,  0.6713,  0.5557,  0.7301,\n",
      "          0.5278, -0.1445],\n",
      "        [ 0.5893, -0.1628,  0.4236,  0.3410,  0.8107, -0.4716,  0.7248, -0.8134,\n",
      "          0.0224,  0.3279],\n",
      "        [ 0.3620,  0.7138,  0.7697, -0.5146, -0.1673, -0.2622, -0.7778,  0.1320,\n",
      "          0.7026, -0.2229]], grad_fn=<TanhBackward>)\n",
      "\n",
      "y2: tensor([[0.6449],\n",
      "        [0.3855],\n",
      "        [0.4551]], grad_fn=<SigmoidBackward>)\n",
      "h2: tensor([[ 0.6774,  0.8148, -0.5690, -0.6745, -0.3968, -0.6490,  0.5755, -0.6310,\n",
      "         -0.3404,  0.7683],\n",
      "        [ 0.1551,  0.3912,  0.3440, -0.6927, -0.4268,  0.1066, -0.6101, -0.7880,\n",
      "          0.7915, -0.0948],\n",
      "        [ 0.7827,  0.5720,  0.7875,  0.2503,  0.0798, -0.5607, -0.0228, -0.6098,\n",
      "          0.4632, -0.7429]], grad_fn=<TanhBackward>)\n",
      "\n",
      "y3: tensor([[0.5147],\n",
      "        [0.5140],\n",
      "        [0.5383]], grad_fn=<SigmoidBackward>)\n",
      "h3: tensor([[ 0.2691,  0.4530, -0.6194,  0.3939,  0.5418, -0.7037, -0.1203,  0.0227,\n",
      "          0.7927, -0.7475],\n",
      "        [ 0.6143,  0.2051, -0.2726,  0.5565,  0.6925, -0.0986,  0.7362,  0.2056,\n",
      "          0.7190, -0.7498],\n",
      "        [ 0.5925,  0.6586,  0.0041,  0.5117,  0.0865, -0.3311,  0.6984, -0.0775,\n",
      "          0.4676, -0.0596]], grad_fn=<TanhBackward>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Manually \"run\" a few time steps\n",
    "\n",
    "# t=1\n",
    "x1 = torch.randn(N, in_dim)\n",
    "y1, h1 = rnn(x1)\n",
    "print(f'y1: {y1}')\n",
    "print(f'h1: {h1}\\n')\n",
    "\n",
    "# t=2\n",
    "x2 = torch.randn(N, in_dim)\n",
    "y2, h2 = rnn(x2, h1)\n",
    "print(f'y2: {y2}')\n",
    "print(f'h2: {h2}\\n')\n",
    "\n",
    "# t=3\n",
    "x3 = torch.randn(N, in_dim)\n",
    "y3, h3 = rnn(x3, h2)\n",
    "print(f'y3: {y3}')\n",
    "print(f'h3: {h3}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1: Sentiment analysis for movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The task: Given a review about a movie written by some user, decide whether it's **positive**, **negative** or **neutral**.\n",
    "\n",
    "<center><img src=\"img/sentiment_analysis.png\" width=\"500\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Classically this is considered a challenging task if approached based on keywords alone.\n",
    "\n",
    "Consider:\n",
    "\n",
    "     \"This movie was actually neither that funny, nor super witty.\"\n",
    "     \n",
    "To comprehend such a sentence, it's intuitive to see that some \"state\" must be kept when \"reading\" it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataset\n",
    "\n",
    "We'll use the [`torchtext`](https://github.com/pytorch/text) package, which provides useful tools for working ith textual data, and also includes some built-in datasets and dataloaders (similar to `torchvision`).\n",
    "\n",
    "Out dataset will be the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/treebank.html) (SST) dataset, which contains ~10,000 **labeled** movie reviews.\n",
    "\n",
    "The label of each review is either \"positive\", \"neutral\" or \"negative\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Loading and tokenizing text samples\n",
    "\n",
    "The `torchtext.data.Field` class takes care of splitting text into unique \"tokens\"\n",
    "(~words) and converting it a numerical representation as a sequence of numbers representing\n",
    "the tokens in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torchtext.data\n",
    "\n",
    "# torchtext Field objects parse text (e.g. a review) and create a tensor representation\n",
    "\n",
    "# This Field object will be used for tokenizing the movie reviews text\n",
    "review_parser = torchtext.data.Field(\n",
    "    sequential=True, use_vocab=True, lower=True,\n",
    "    init_token='<sos>', eos_token='<eos>', dtype=torch.long,\n",
    "    tokenize='spacy', tokenizer_language='en_core_web_sm'\n",
    ")\n",
    "\n",
    "# This Field object converts the text labels into numeric values (0,1,2)\n",
    "label_parser = torchtext.data.Field(\n",
    "    is_target=True, sequential=False, unk_token=None, use_vocab=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 8544\n",
      "Number of test     samples: 2210\n"
     ]
    }
   ],
   "source": [
    "import torchtext.datasets\n",
    "\n",
    "# Load SST, tokenize the samples and labels\n",
    "# ds_X are Dataset objects which will use the parsers to return tensors\n",
    "ds_train, ds_valid, ds_test = torchtext.datasets.SST.splits(\n",
    "    review_parser, label_parser, root=data_dir\n",
    ")\n",
    "\n",
    "n_train = len(ds_train)\n",
    "print(f'Number of training samples: {n_train}')\n",
    "print(f'Number of test     samples: {len(ds_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lets print some examples from our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample#0111 [positive]:\n",
      " the film aims to be funny , uplifting and moving , sometimes all at once .\n",
      "\n",
      "sample#4321 [neutral ]:\n",
      " the most anti - human big studio picture since 3000 miles to graceland .\n",
      "\n",
      "sample#7777 [negative]:\n",
      " an ugly , revolting movie .\n",
      "\n",
      "sample#0000 [positive]:\n",
      " the rock is destined to be the 21st century 's new ` ` conan '' and that he 's going to make a splash even greater than arnold schwarzenegger , jean - claud van damme or steven segal .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in ([111, 4321, 7777, 0]):\n",
    "    example = ds_train[i]\n",
    "    label = example.label\n",
    "    review = str.join(\" \", example.text)\n",
    "    print(f'sample#{i:04d} [{label:8s}]:\\n {review}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Building a vocabulary\n",
    "\n",
    "The `Field` object can build a **vocabulary** for us,\n",
    "which is simply a bi-directional mapping between a unique index and a token.\n",
    "\n",
    "We'll only include words from the training set in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in training samples: 15482\n",
      "Number of tokens in training labels: 3\n"
     ]
    }
   ],
   "source": [
    "review_parser.build_vocab(ds_train)\n",
    "label_parser.build_vocab(ds_train)\n",
    "\n",
    "print(f\"Number of tokens in training samples: {len(review_parser.vocab)}\")\n",
    "print(f\"Number of tokens in training labels: {len(label_parser.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 20 tokens:\n",
      " ['<unk>', '<pad>', '<sos>', '<eos>', '.', 'the', ',', 'a', 'and', 'of', 'to', '-', 'is', \"'s\", 'it', 'that', 'in', 'as', 'but', 'film']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'first 20 tokens:\\n', review_parser.vocab.itos[:20], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note the **special tokens**, `<unk>`, `<pad>`, `<sos>` and `<eos>` at indexes `0-3`.\n",
    "These were automatically created by the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word=film            index=19\n",
      "word=actor           index=492\n",
      "word=schwarzenegger  index=3404\n",
      "word=spielberg       index=715\n"
     ]
    }
   ],
   "source": [
    "# Show that some words exist in the vocab\n",
    "for w in ['film', 'actor', 'schwarzenegger', 'spielberg']:\n",
    "    print(f'word={w:15s} index={review_parser.vocab.stoi[w]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels vocab:\n",
      " {'positive': 0, 'negative': 1, 'neutral': 2}\n"
     ]
    }
   ],
   "source": [
    "print(f'labels vocab:\\n', dict(label_parser.vocab.stoi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Data loaders (iterators)\n",
    "\n",
    "The `torchtext` package comes with `Iterator`s, similar to the `DataLoaders` we previously worked with.\n",
    "\n",
    "A key issue when working with text sequences is that each sample is of a different length.\n",
    "\n",
    "So, how can we work with **batches** of data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "# BucketIterator is supposed to created batches with samples of similar length\n",
    "# to minimize the number of <pad> tokens in the batch.\n",
    "dl_train, dl_valid, dl_test = torchtext.data.BucketIterator.splits(\n",
    "    (ds_train, ds_valid, ds_test), batch_size=BATCH_SIZE,\n",
    "    shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lets look at a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = \n",
      " tensor([[    2,     2,     2,     2],\n",
      "        [   14,  9673,   465,    20],\n",
      "        [   89, 13414,    51,  2741],\n",
      "        [  210,   493,    32,  1746],\n",
      "        [    7,    12,    26,     6],\n",
      "        [ 1026,     7,  1483,   252],\n",
      "        [12398, 11221,   333,  1168],\n",
      "        [   10,    19,  1481,    89],\n",
      "        [ 2671,    15,    13,    29],\n",
      "        [   60,  2350, 10668,   192],\n",
      "        [    7,     7,     6,     6],\n",
      "        [  182,    63,     5,    18],\n",
      "        [  692,  1840,    19,   968],\n",
      "        [    9,    30,  8425,     4],\n",
      "        [  653,  8907,    24,     3],\n",
      "        [ 2205,     9,  2514,     1],\n",
      "        [   13,     5,   316,     1],\n",
      "        [  449,   169,  1152,     1],\n",
      "        [  569,    13,    17,     1],\n",
      "        [    4, 10382,     5,     1],\n",
      "        [    3,  7537,   836,     1],\n",
      "        [    1,    41,     6,     1],\n",
      "        [    1,     8,  2793,     1],\n",
      "        [    1,    14,   333,     1],\n",
      "        [    1,  1050,  1481,     1],\n",
      "        [    1,     5,  1812,     1],\n",
      "        [    1,   747,    10,     1],\n",
      "        [    1,    88,   911,     1],\n",
      "        [    1,     5,   471,     1],\n",
      "        [    1,  4132,    30,     1],\n",
      "        [    1,     4,   138,     1],\n",
      "        [    1,     3,     8,     1],\n",
      "        [    1,     1,    62,     1],\n",
      "        [    1,     1,   938,     1],\n",
      "        [    1,     1,    15,     1],\n",
      "        [    1,     1,    89,     1],\n",
      "        [    1,     1,    87,     1],\n",
      "        [    1,     1,  2741,     1],\n",
      "        [    1,     1,   388,     1],\n",
      "        [    1,     1,   485,     1],\n",
      "        [    1,     1,    21,     1],\n",
      "        [    1,     1,  3052,     1],\n",
      "        [    1,     1,     4,     1],\n",
      "        [    1,     1,     3,     1]]) torch.Size([44, 4])\n",
      "\n",
      "y = \n",
      " tensor([1, 0, 0, 2]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dl_train))\n",
    "\n",
    "X, y = batch.text, batch.label\n",
    "print('X = \\n', X, X.shape, end='\\n\\n')\n",
    "print('y = \\n', y, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What are we looking at?\n",
    "\n",
    "Our sample tensor `X` is of shape `(sentence_length, batch_size)`.\n",
    "\n",
    "Note that:\n",
    "1. `sentence_length` changes every batch!\n",
    "2. Sequence dimension first (not batch). This is the convention with RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model\n",
    "\n",
    "We'll now create our sentiment analysis model based on the simple `RNNLayer` we've implemented above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model will:\n",
    "- Take an input batch of tokenized sentences.\n",
    "- Compute a dense word-embedding of each token.\n",
    "- Process the sentence **sequentially** through the RNN layer.\n",
    "- Produce a `(B, 3)` tensor, which we'll interpret as class probabilities for each sentence in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is a **word embedding**? How do we get one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Embeddings encode tokens as tensors in a way that maintain some **semantic** meaning for our task.\n",
    "\n",
    "Generally we should take a pre-trained embedding depending on our task type.\n",
    "\n",
    "Here we'll train an Embedding together with our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 4, 1, 4, 2, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6953, -0.7679, -0.0966,  1.6150,  0.5369,  0.5796,  0.9543, -0.0991],\n",
       "        [ 0.6953, -0.7679, -0.0966,  1.6150,  0.5369,  0.5796,  0.9543, -0.0991],\n",
       "        [ 0.6526, -1.4069,  0.5882,  0.6547,  1.6852,  1.7977,  0.1934, -1.1658],\n",
       "        [ 0.6953, -0.7679, -0.0966,  1.6150,  0.5369,  0.5796,  0.9543, -0.0991],\n",
       "        [-0.2869, -0.1085, -1.4349, -0.2508, -0.5541, -0.0988, -1.0129,  0.3338],\n",
       "        [ 0.6526, -1.4069,  0.5882,  0.6547,  1.6852,  1.7977,  0.1934, -1.1658]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = nn.Embedding(num_embeddings=5, embedding_dim=8)\n",
    "\n",
    "token_idx = torch.randint(low=0, high=5, size=(6,))\n",
    "print(token_idx)\n",
    "embedding_layer(token_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "OK, model time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, in_dim, embedding_dim, h_dim, out_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # nn.Embedding converts from token index to dense tensor\n",
    "        self.embedding = nn.Embedding(in_dim, embedding_dim)\n",
    "        \n",
    "        # Our own Vanilla RNN layer, without phi_y so it outputs a class score\n",
    "        self.rnn = RNNLayer(embedding_dim, h_dim, out_dim, phi_y=None)\n",
    "        \n",
    "        # To convert class scores to log-probability we'll apply log-softmax\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # X shape: (S, B) Note batch dim is not first!\n",
    "        \n",
    "        embedded = self.embedding(X) # embedded shape: (S, B, E)\n",
    "        \n",
    "        # Loop over (batch of) tokens in the sentence(s)\n",
    "        ht = None\n",
    "        for xt in embedded:\n",
    "            yt, ht = self.rnn(xt, ht) # yt is (B, D_out)\n",
    "        \n",
    "        # Class scores to log-probability\n",
    "        yt_log_proba = self.log_softmax(yt)\n",
    "        \n",
    "        return yt_log_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this model, what should the `in_dim` be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(15482, 100)\n",
       "  (rnn): RNNLayer(\n",
       "    (fc_xh): Linear(in_features=100, out_features=128, bias=False)\n",
       "    (fc_hh): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (fc_hy): Linear(in_features=128, out_features=3, bias=True)\n",
       "  )\n",
       "  (log_softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = len(review_parser.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 3\n",
    "\n",
    "model = SentimentRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Test a manual forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model(X) = \n",
      " tensor([[-0.6577, -1.8009, -1.1494],\n",
      "        [-0.6577, -1.8009, -1.1494],\n",
      "        [-1.2067, -1.1303, -0.9732],\n",
      "        [-0.6577, -1.8009, -1.1494]], grad_fn=<LogSoftmaxBackward>) torch.Size([4, 3])\n",
      "labels =  tensor([1, 0, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "print(f'model(X) = \\n', model(X), model(X).shape)\n",
    "print(f'labels = ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How big is our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RNN model has 1,577,899 trainable weights.\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The RNN model has {count_parameters(model):,} trainable weights.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why so many? We used only one RNN layer.\n",
    "\n",
    "Where are most of the weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training\n",
    "\n",
    "Let's complete the example by showing the regular pytorch-style train loop with this model.\n",
    "\n",
    "We'll run only a few epochs on a small subset just to test that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, dataloader, max_epochs=4, max_batches=200):\n",
    "    for epoch_idx in range(max_epochs):\n",
    "        total_loss, num_correct = 0, 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            X, y = batch.text, batch.label\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred_log_proba = model(X)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(y_pred_log_proba, y)\n",
    "            loss.backward()\n",
    "\n",
    "            # Weight updates\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            total_loss += loss.item()\n",
    "            y_pred = torch.argmax(y_pred_log_proba, dim=1)\n",
    "            num_correct += torch.sum(y_pred == y).float().item()\n",
    "\n",
    "            if batch_idx == max_batches-1:\n",
    "                break\n",
    "                \n",
    "        print(f\"Epoch #{epoch_idx}, loss={total_loss /(max_batches):.3f}, accuracy={num_correct /(max_batches*BATCH_SIZE):.3f}, elapsed={time.time()-start_time:.1f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0, loss=1.109, accuracy=0.398, elapsed=3.4 sec\n",
      "Epoch #1, loss=1.106, accuracy=0.372, elapsed=3.4 sec\n",
      "Epoch #2, loss=1.070, accuracy=0.421, elapsed=3.5 sec\n",
      "Epoch #3, loss=1.061, accuracy=0.438, elapsed=3.8 sec\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "rnn_model = SentimentRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
    "\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Recall: LogSoftmax + NLL is equiv to CrossEntropy on the class scores\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "train(rnn_model, optimizer, loss_fn, dl_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Limitations\n",
    "\n",
    "As usual this is a very na√Øve model, just for demonstration.\n",
    "It lacks many tricks of the NLP trade, such was pre-trained embeddings,\n",
    "gated RNN units, deep or bi-directional models, dropout, etc.\n",
    "\n",
    "Don't expect SotA results :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2: Temporal Convolution Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "RNNs are sometimes useful but they have some serious drawbacks:\n",
    "\n",
    "1. Vanilla RNNs are very hard to train on long sequences (large `S`) and thus are almost never used.\n",
    "Instead, more complex RNNs like LSTMs and GRUs are employed (See HW3 :).\n",
    "\n",
    "2. They require processing the input **sequentially** which results in poor performance for both training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But CNNs are great! Can we use them instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "OK, but how do we deal with:\n",
    "1. Input and output must both be sequences of same length, which should be arbitrary.\n",
    "1. Output at time $t$ only depends on inputs up to time $t$.\n",
    "1. Long-term dependencies across time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Input and output must both be sequences of same length, which should be arbitrary.<br>\n",
    "   $\\Longrightarrow$ 1D fully-convolutional architecture with appropriate **padding**.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Output at time $t$ only depends on inputs up to time $t$.<br>\n",
    "    $\\Longrightarrow$ **Causal convolutions**, where an output at time $t$ is convolved only with elements from time $t$ and earlier in the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Long-term dependencies across time.<br>\n",
    "    $\\Longrightarrow$ Increase receptive field using **depth** (facilitated by residual connections) and **dilated convolutions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Combining these techniques gives us the TCN architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/tcn.png\" width=\"1400\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's implement a first a **general** TCN residual block and then a full TCN model for our Sentiment Analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, padding, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=padding, dilation=dilation),\n",
    "            nn.ReLU(), nn.Dropout(dropout)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size, stride=1, padding=padding, dilation=dilation),\n",
    "            nn.ReLU(), nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.channels_adapter = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.channels_adapter = None\n",
    "            \n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Main branch\n",
    "        out = self.conv1(x)\n",
    "        out = out[..., :-self.padding] # maintain causality\n",
    "        out = self.conv2(out)\n",
    "        out = out[..., :-self.padding]\n",
    "\n",
    "        # Skip-connection (residual branch)\n",
    "        skip = x if not self.channels_adapter else self.channels_adapter(x)\n",
    "            \n",
    "        out = torch.relu(out + skip)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentTCN(nn.Module):\n",
    "    def __init__(self, in_dim: int, embedding_dim: int, layer_channels: list, out_dim: int, kernel_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        assert len(layer_channels) > 0\n",
    "\n",
    "        self.embedding = nn.Embedding(in_dim, embedding_dim)\n",
    "        \n",
    "        tcn_channels = [embedding_dim] + layer_channels + [out_dim]\n",
    "        layers = []\n",
    "        for i, (c_in, c_out) in enumerate(zip(tcn_channels[:-1], tcn_channels[1:])):\n",
    "            # Exponentially-increasing dilation\n",
    "            dilation = 2 ** i\n",
    "            # Control padding to maintain output size\n",
    "            padding = (kernel_size - 1) * dilation\n",
    "            \n",
    "            layers.append(\n",
    "                TCNBlock(c_in, c_out, kernel_size, dilation=dilation, padding=padding, dropout=dropout)\n",
    "            )\n",
    "            \n",
    "        self.blocks = nn.Sequential(*layers)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, **kw): # x is (S, B)\n",
    "        # First we need to embed our sequence.\n",
    "        # Note how we treat the E as channels for the convulutions.\n",
    "        x_emb = self.embedding(x) # (S, B, E)\n",
    "        x_emb = torch.transpose(x_emb, 0, 1) # (B, S, E)\n",
    "        x_emb = torch.transpose(x_emb, 1, 2) # (B, E, S)\n",
    "        \n",
    "        # Process the entire sequence at once\n",
    "        y_seq = self.blocks(x_emb) # (B, D_out, S)\n",
    "        \n",
    "        # Output predictions\n",
    "        yt = y_seq[..., -1] # (B, D_out)\n",
    "        yt_log_proba = self.log_softmax(yt)\n",
    "        return yt_log_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentTCN(\n",
       "  (embedding): Embedding(15482, 100)\n",
       "  (blocks): Sequential(\n",
       "    (0): TCNBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv1d(100, 32, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (channels_adapter): Conv1d(100, 32, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (1): TCNBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): TCNBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv1d(32, 3, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv1d(3, 3, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (channels_adapter): Conv1d(32, 3, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (log_softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcn = SentimentTCN(INPUT_DIM, EMBEDDING_DIM, [32, 32], OUTPUT_DIM, kernel_size=3)\n",
    "tcn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice where the `channel_adapter`s are. What's their purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TCN model has 1,570,796 trainable weights.\n"
     ]
    }
   ],
   "source": [
    "print(f'The TCN model has {count_parameters(tcn):,} trainable weights.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try a forward pass with this new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7623, -1.2240, -1.4298],\n",
       "        [-0.9873, -1.0202, -1.3209],\n",
       "        [-1.4318, -0.6981, -1.3333],\n",
       "        [-0.8065, -1.1583, -1.4288]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcn(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And finally, let's train the new model using the exact same setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0, loss=1.092, accuracy=0.367, elapsed=3.3 sec\n",
      "Epoch #1, loss=1.075, accuracy=0.393, elapsed=3.2 sec\n",
      "Epoch #2, loss=1.070, accuracy=0.371, elapsed=3.2 sec\n",
      "Epoch #3, loss=1.063, accuracy=0.395, elapsed=3.4 sec\n"
     ]
    }
   ],
   "source": [
    "tcn_model = SentimentTCN(INPUT_DIM, EMBEDDING_DIM, [32, 32], OUTPUT_DIM, kernel_size=3)\n",
    "optimizer = optim.Adam(tcn_model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "train(tcn_model, optimizer, loss_fn, dl_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice how:\n",
    "1. The model took sequences of different length every batch, just like an RNN.\n",
    "2. Sequences were processed in **parallel** w.r.t. time, unlike RNN.\n",
    "3. Receptive field was controlled by architecture, not by sequence length, unlike RNN.\n",
    "4. No need for BPTT, unlike RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Image credits**\n",
    "\n",
    "Some images in this tutorial were taken and/or adapted from:\n",
    "\n",
    "- Fundamentals of Deep Learning, Nikhil Buduma, Oreilly 2017\n",
    "- Andrej Karpathy, http://karpathy.github.io\n",
    "- MIT 6.S191\n",
    "- Stanford cs231n\n",
    "- S. Bai et al. 2018, http://arxiv.org/abs/1803.01271"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
